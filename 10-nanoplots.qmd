# Nanoplots {#sec-nanoplots}

```{r setup, include=FALSE, echo=FALSE}
library(gt)
library(dplyr)
library(tidyr)
```

Tables present numbers; charts reveal patterns. But what if a table could do both? Nanoplots (tiny, embedded visualizations) bridge this divide by placing miniature graphics directly within table cells. These small-scale data representations can show trends, distributions, and comparisons at a glance, augmenting numeric values with visual context that aids rapid comprehension.

The term "sparkline" (coined by Edward Tufte) describes "data-intense, design-simple, word-sized graphics" that can be embedded inline with text. Nanoplots in **gt** extend this concept to tabular contexts, allowing each row to carry its own miniature visualization derived from the row's data. They are deliberately simple (there's limited space, after all) but that simplicity becomes a virtue. A nanoplot doesn't replace detailed analysis; it provides an immediate visual summary that guides the reader's attention and facilitates comparison across rows.

This chapter explores the `cols_nanoplot()` function and its companion `nanoplot_options()` helper. We'll examine the three available plot types (line plots, bar plots, and box plots) and discover how to customize their appearance, handle missing data, add reference elements, and supply data in various formats. By the chapter's end, you'll be equipped to enhance your tables with these compact but powerful visualizations.

## The `cols_nanoplot()` function

The `cols_nanoplot()` function creates a new column containing nanoplots, using data from one or more existing columns. The function collects numeric values across specified columns for each row, generates a plot from those values, and places the resulting visualization in a new column.

**Function Signature**

```r
cols_nanoplot(
  data,
  columns,
  rows = everything(),
  plot_type = c("line", "bar", "boxplot"),
  plot_height = "2em",
  missing_vals = c("gap", "marker", "zero", "remove"),
  autoscale = FALSE,
  autohide = TRUE,
  columns_x_vals = NULL,
  reference_line = NULL,
  reference_area = NULL,
  expand_x = NULL,
  expand_y = NULL,
  new_col_name = NULL,
  new_col_label = NULL,
  before = NULL,
  after = NULL,
  options = NULL
)
```

Let's start with a simple example using the `illness` dataset, which contains daily medical test measurements:

```{r}
illness |>
  dplyr::slice_head(n = 8) |>
  gt(rowname_col = "test") |>
  tab_header(title = "Daily Medical Test Results") |>
  cols_hide(columns = c(starts_with("norm"), units)) |>
  cols_nanoplot(
    columns = starts_with("day"),
    new_col_name = "trend",
    new_col_label = "7-Day Trend"
  )
```

This table shows medical test measurements with a line plot summarizing the seven daily values. The `columns = starts_with("day")` argument collects data from all columns beginning with "day", concatenating them left-to-right to form each row's data series. The source columns are automatically hidden (due to `autohide = TRUE` by default), keeping the table clean.

The nanoplot provides immediate visual context: readers can instantly see whether values trended upward, downward, or remained stable, without mentally parsing seven separate numbers.

Two arguments control how the new nanoplot column is identified and presented. The `new_col_name = "trend"` argument sets the internal column name and this is what you'll use to reference this column in subsequent **gt** operations like `cols_width()`, `cols_align()`, `cols_move()`, or `tab_style()`. Think of it as the column's programmatic identifier. Meanwhile, `new_col_label = "7-Day Trend"` sets what readers actually see in the column header. This label can include spaces, special characters, or even markdown formatting (via `md()`), making it reader-friendly while keeping the internal name concise and code-friendly.

Both arguments are optional. If you omit `new_col_name`, **gt** generates a default name (typically "nanoplots"). If you omit `new_col_label`, the column name itself becomes the label. However, providing explicit values is good practice: it makes your code clearer and ensures column headers communicate effectively with your audience. You'll often want a short, simple internal name for coding convenience paired with a more descriptive, formatted label for presentation clarity.

### Data input: columns vs. value streams

Nanoplots accept data in two flexible formats, allowing you to work with whatever structure your data already has.

The first format spreads values across multiple columns, as we just saw with the `illness` dataset. Each column contributes one value to the nanoplot, collected in left-to-right order:

```{r}
# Multi-column format: each column is one value
dplyr::tibble(
  product = c("Widget", "Gadget", "Gizmo"),
  q1_sales = c(120, 85, 210),
  q2_sales = c(135, 92, 198),
  q3_sales = c(142, 88, 225),
  q4_sales = c(156, 95, 245)
) |>
  gt(rowname_col = "product") |>
  tab_header(title = "Quarterly Sales") |>
  cols_nanoplot(
    columns = starts_with("q"),
    new_col_name = "trend",
    new_col_label = "Quarterly Trend"
  ) |>
  cols_width(trend ~ px(100))
```

The second format packs all values for a row into a single column as a delimited string. This is particularly useful when different rows have varying numbers of data points, or when your data arrives in this format from external sources:

```{r}
# Value stream format: comma-separated values in one column
dplyr::tibble(
  product = c("Widget", "Gadget", "Gizmo"),
  sales_stream = c("120, 135, 142, 156", "85, 92, 88, 95", "210, 198, 225, 245")
) |>
  gt(rowname_col = "product") |>
  tab_header(title = "Quarterly Sales") |>
  cols_nanoplot(
    columns = sales_stream,
    new_col_name = "trend",
    new_col_label = "Quarterly Trend"
  ) |>
  cols_width(trend ~ px(100))
```

Both formats produce identical nanoplots. The multi-column approach works well when your table already has the data structured that way and you want to show those columns alongside the nanoplot. The value stream approach is good for when you need flexibility in the number of data points per row or want to keep your table structure simple. Commas, spaces, or semicolons can all serve as delimiters in value streams.

## Line plots

Line plots are the default nanoplot type and is best for showing trends over ordered sequences. They consist of three visual layers that can be independently controlled: data points (the actual values), a connecting line, and a filled area beneath the line.

### Basic line plots

Line plots are the natural choice when your data represents a continuous progression or ordered sequence. Time series, sequential measurements, cumulative values, and any data where the order matters and you want to emphasize change from one value to the next: all of these benefit from line plot representation. The connecting line creates a visual path that guides the eye through the progression, making it easy to spot upward trends, downward trends, plateaus, or sudden changes.

In the context of nanoplots, line plots serve as compact trend indicators that answer questions like "Is this going up or down?" and "How volatile is this pattern?" at a glance. Because they're the default plot type, you don't need to specify `plot_type = "line"`. Simply calling `cols_nanoplot()` produces a line plot. This makes them the quickest option when you need a basic visual summary without customization.

The `towny` dataset contains population data for municipalities across multiple census years. By selecting columns that start with "population", we collect a time series for each municipality that spans several decades. The resulting line plots provide an immediate visual history of population growth:

```{r}
towny |>
  dplyr::select(name, starts_with("population")) |>
  dplyr::slice_max(population_2021, n = 8) |>
  gt() |>
  fmt_integer(columns = starts_with("population")) |>
  cols_nanoplot(
    columns = starts_with("population"),
    new_col_name = "pop_trend",
    new_col_label = "Population Trend"
  ) |>
  cols_hide(columns = matches("199|200|201")) |>
  cols_width(pop_trend ~ px(120))
```

Each municipality's population history spanning decades condenses into a compact visualization. The line plot reveals growth trajectories that would require careful numeric comparison to discern from the raw numbers alone. You can immediately see which municipalities experienced steady growth (smooth upward slopes), which grew explosively in recent decades (sharp upward curves), and which remained relatively stable (nearly flat lines).

The visual comparison across rows is particularly valuable here. Without the nanoplots, determining which municipality grew fastest would require mentally calculating growth rates from the numeric columns. The line plots make this comparison instant as steeper slopes indicate faster growth. Similarly, you can spot patterns like early rapid growth followed by stabilization, or slow initial growth accelerating in later years, patterns that might be missed when scanning columns of numbers.

By hiding the intermediate year columns with `cols_hide(columns = matches("199|200|201"))`, we keep the table clean while still showing the most recent population figure (2021) alongside the visual trend. This combination of current value and historical trend provides both the "what is it now?" and "how did we get here?" perspectives in a single glance.

### Customizing line plot appearance

The `nanoplot_options()` helper function provides extensive control over visual properties. Let's create line plots with customized styling:
 
```{r}
towny |>
  dplyr::select(name, starts_with("density")) |>
  dplyr::slice_max(density_2021, n = 6) |>
  gt() |>
  fmt_number(columns = starts_with("density"), decimals = 0) |>
  cols_nanoplot(
    columns = starts_with("density"),
    new_col_name = "density_trend",
    new_col_label = "Density Over Time",
    options = nanoplot_options(
      data_point_radius = 6,
      data_point_stroke_color = "darkblue",
      data_point_stroke_width = 2,
      data_point_fill_color = "lightblue",
      data_line_stroke_color = "steelblue",
      data_line_stroke_width = 3,
      data_area_fill_color = "lightsteelblue"
    )
  ) |>
  cols_hide(columns = matches("199|200|201")) |>
  cols_width(density_trend ~ px(140))
```

The blue color scheme creates visual cohesion. Larger data points with contrasting stroke and fill colors improve visibility, while the wider line and lighter area fill create depth.

### Showing only specific layers

Sometimes you want to emphasize particular aspects of the data. Here's a line-only plot without points or area:

```{r}
sp500 |>
  dplyr::filter(date >= "2015-01-01" & date <= "2015-03-31") |>
  dplyr::select(date, close) |>
  dplyr::mutate(month = format(date, "%Y-%m")) |>
  dplyr::summarize(
    prices = paste(close, collapse = ","),
    .by = month
  ) |>
  gt(rowname_col = "month") |>
  tab_header(title = "S&P 500 Daily Closing Prices", subtitle = "Q1 2015") |>
  cols_nanoplot(
    columns = prices,
    new_col_name = "price_chart",
    new_col_label = "Daily Prices",
    options = nanoplot_options(
      show_data_points = FALSE,
      show_data_area = FALSE,
      data_line_stroke_color = "#2E7D32",
      data_line_stroke_width = 2,
      data_line_type = "straight"
    )
  ) |>
  cols_width(price_chart ~ px(200))
```

With many data points, hiding individual markers reduces visual clutter. The `data_line_type = "straight"` option uses straight line segments instead of curves, which can be clearer for volatile data.

Alternatively, show only points for a scatter-like appearance:

```{r}
exibble |>
  dplyr::select(row, group, num, currency) |>
  gt(rowname_col = "row", groupname_col = "group") |>
  cols_nanoplot(
    columns = c(num, currency),
    new_col_name = "values",
    new_col_label = "Values",
    options = nanoplot_options(
      show_data_line = FALSE,
      show_data_area = FALSE,
      data_point_radius = 8,
      data_point_fill_color = "coral",
      data_point_stroke_color = "darkred",
      data_point_stroke_width = 2
    )
  )
```

With only two data points per row, a connecting line adds little value. Points alone clearly show the two values and their relative magnitudes.

### Line plots with x-axis values

By default, nanoplots space data points evenly along the x-axis. For line plots, you can supply explicit x values using `columns_x_vals`:

```{r}
# Create sample data with irregular time intervals
dplyr::tibble(
  category = c("Product A", "Product B", "Product C"),
  times = c("1,3,4,8,12", "2,5,6,9,15", "1,2,7,10,14"),
  values = c("10,15,13,20,25", "8,12,15,11,18", "5,8,12,15,20")
) |>
  gt(rowname_col = "category") |>
  tab_header(title = "Sales at Irregular Intervals") |>
  cols_nanoplot(
    columns = values,
    columns_x_vals = times,
    new_col_name = "trend",
    new_col_label = "Sales Trend",
    expand_x = c(0, 16)
  ) |>
  cols_width(trend ~ px(150))
```

The x values position points according to their actual timing rather than equally spacing them. The `expand_x` argument ensures all plots share the same x-axis range for valid comparison.

## Bar plots

Bar plots are good when showing categorical comparisons and clearly distinguishing positive from negative values. Unlike line plots, bar plots always use equal spacing and ignore any x-axis values.

### Basic bar plots

Bar plots work best when you need to compare discrete categories or sequential values where the magnitude of each individual item matters. Each bar represents a single data point, and its height encodes the value. When multiple values appear in a single nanoplot, the bars stand side by side, making it easy to compare magnitudes both within a row (across bars) and between rows (comparing corresponding bars).

In the context of tables, bar nanoplots are particularly effective for showing compositions, breakdowns, or multi-part measurements. For instance, if each row represents a different entity and the columns contain related metrics, a bar nanoplot can show all those metrics together in a compact visual form. This allows readers to quickly grasp not just which values are largest or smallest, but how the pattern of relative magnitudes differs from row to row.

```{r}
pizzaplace |>
  dplyr::count(type, date) |>
  tidyr::pivot_wider(names_from = type, values_from = n) |>
  dplyr::slice_head(n = 7) |>
  gt(rowname_col = "date") |>
  tab_header(title = "Daily Pizza Sales by Type") |>
  fmt_date(columns = stub(), date_style = "MMMd") |>
  cols_nanoplot(
    columns = c(chicken, classic, supreme, veggie),
    plot_type = "bar",
    new_col_name = "by_type",
    new_col_label = "Sales Distribution"
  ) |>
  cols_width(by_type ~ px(100))
```

Each bar represents one pizza type's sales for that day. The relative heights reveal the sales mix, showing which types dominated each day's orders. By comparing across rows, you can see whether certain days had notably different sales patterns. For example, a day where the classic pizza bar is much taller than the others indicates strong preference for that type, while more uniform bar heights suggest balanced sales across types.

The bars are positioned in the same order as the columns specified in the `columns` argument, creating a consistent visual structure. This consistency allows your eye to track a specific category across multiple rows. If the veggie pizza is always the fourth bar, you can quickly scan down the column to assess veggie pizza sales across all days without having to reorient yourself for each row.

Bar plots also make zero values and missing values visually obvious. A missing bar (or a bar with zero height) stands out immediately, drawing attention to gaps in the data. This is different from line plots, where missing values might create subtle gaps that could be overlooked.

### Customizing bar colors

When bar plots display multiple categories, assigning distinct colors to each bar position creates immediate visual differentiation. Instead of relying on position alone, color allows readers to identify specific categories at a glance. This technique is particularly effective when the categories have inherent associations (like product types, regions, or departments) that benefit from consistent color coding.

The `data_bar_fill_color` option in `nanoplot_options()` accepts a vector of colors, with each color corresponding to a bar position in the order specified by the `columns` argument. The first color applies to the first column's bar, the second color to the second column's bar, and so on. This positional consistency means that across all rows, the same category always appears in the same color, creating a visual legend that persists throughout the table.

When using multiple colors, providing a legend is essential. Without one, readers must deduce the color-to-category mapping by examining the source columns (if visible) or through trial and error. A clear legend, whether in a footnote, source note, or table caption, eliminates ambiguity and ensures readers can interpret the colored bars correctly from the first glance.

```{r}
pizzaplace |>
  dplyr::count(type, date) |>
  tidyr::pivot_wider(names_from = type, values_from = n) |>
  dplyr::slice_head(n = 7) |>
  gt(rowname_col = "date") |>
  tab_header(title = "Daily Pizza Sales by Type") |>
  fmt_date(columns = stub(), date_style = "MMMd") |>
  fmt_integer(columns = c(chicken, classic, supreme, veggie)) |>
  cols_align(align = "center", columns = everything()) |>
  cols_nanoplot(
    columns = c(chicken, classic, supreme, veggie),
    plot_type = "bar",
    autohide = FALSE,
    new_col_name = "by_type",
    new_col_label = "Sales by Type",
    options = nanoplot_options(
      data_bar_stroke_color = "transparent",
      data_bar_fill_color = c("#D35400", "#F4D03F", "#8E44AD", "#27AE60")
    )
  ) |>
  cols_width(everything() ~ px(100)) |>
  tab_source_note(
    source_note = md(paste0(
      "<span style=\"color:#D35400;\">&#9632;</span> Chicken &nbsp;&nbsp; ",
      "<span style=\"color:#F4D03F;\">&#9632;</span> Classic &nbsp;&nbsp; ",
      "<span style=\"color:#8E44AD;\">&#9632;</span> Supreme &nbsp;&nbsp; ",
      "<span style=\"color:#27AE60;\">&#9632;</span> Veggie"
    ))
  )
```

With `autohide = FALSE`, the source columns remain visible, allowing readers to see both exact numbers and the visual comparison. The colored bars create an instant visual signature for each pizza type. The source note uses HTML to create colored squares (&#9632; is the Unicode character for a square) that match the bar colors, providing an unambiguous legend without requiring readers to cross-reference column positions.

The even column widths (achieved with `cols_width(everything() ~ px(100))`) and centered alignment create a balanced, symmetric layout. This uniformity emphasizes the visual comparison by removing layout-based distractions. When all elements are equally spaced and aligned, differences in bar heights become the dominant visual feature.

Color choice matters. The colors used here have strong contrast and distinct hues (orange, yellow, purple, green), making them easy to distinguish even for readers with some forms of color vision deficiency. Avoid using colors that differ only in saturation or lightness, as these can be difficult to differentiate. Test your color palette to ensure sufficient contrast between adjacent bars.

### Bar plots with positive and negative values

When data contains both positive and negative values, bar plots automatically apply different visual styling to distinguish them. This is particularly useful for displaying changes, differences, or variance metrics where direction matters as much as magnitude. Positive values might represent growth, gains, or increases, while negative values indicate declines, losses, or decreases.

By default, **gt** renders positive and negative bars with distinct fill colors, making the directional information immediately visible without requiring readers to examine numeric values or axis labels. You can customize these colors using the `data_bar_fill_color` and `data_bar_negative_fill_color` options in `nanoplot_options()`, allowing you to align the color scheme with your data's semantics. For instance, green for positive changes and red for negative changes, or any other color pairing that suits your context:

```{r}
# Create data with positive and negative changes
dplyr::tibble(
  metric = c("Revenue", "Costs", "Margin", "Volume"),
  q1_change = c(12.5, -3.2, 8.1, -1.5),
  q2_change = c(-2.1, 5.8, -4.3, 6.2),
  q3_change = c(7.8, -1.9, 3.2, -2.8),
  q4_change = c(4.2, 2.1, -1.5, 8.9)
) |>
  gt(rowname_col = "metric") |>
  tab_header(title = "Quarterly Percent Changes") |>
  cols_nanoplot(
    columns = ends_with("change"),
    plot_type = "bar",
    new_col_name = "quarterly",
    new_col_label = "Q1–Q4 Changes",
    options = nanoplot_options(
      data_bar_fill_color = "#2ECC71",
      data_bar_stroke_color = "#1E8449",
      data_bar_negative_fill_color = "#E74C3C",
      data_bar_negative_stroke_color = "#922B21"
    )
  ) |>
  cols_width(quarterly ~ px(120))
```

Positive changes appear in green while negative changes display in red, making it immediately apparent which quarters saw gains versus losses for each metric.

### Horizontal reference lines in bar plots

Reference lines add analytical context to bar plots by marking specific values of interest. They can highlight thresholds, targets, or statistical measures like means and medians. When comparing bars across different positions or rows, a reference line provides a common benchmark that makes it easier to assess whether individual values exceed, fall short of, or align with a particular standard.

You can specify reference lines using keywords (like `"mean"` or `"median"`) to compute values from the data itself, or supply fixed numeric values when you have predetermined thresholds or targets in mind. The reference line appears as a horizontal line across the plot, typically in a contrasting color to ensure visibility against the bars:

```{r}
countrypops |>
  dplyr::filter(
    country_name %in% c("India", "China", "Nigeria", "Brazil", "Japan"),
    year >= 1960,
    year %% 10 == 0
  ) |>
  dplyr::select(country_name, year, population) |>
  dplyr::mutate(pop_millions = population / 1e6, .keep = "unused") |>
  tidyr::pivot_wider(names_from = year, values_from = pop_millions) |>
  gt(rowname_col = "country_name") |>
  tab_header(title = "Population Trends (Millions)", subtitle = "1960–2020") |>
  cols_nanoplot(
    columns = where(is.numeric),
    plot_type = "bar",
    reference_line = "mean",
    new_col_name = "pop_bars",
    new_col_label = "Annual Population"
  ) |>
  cols_width(pop_bars ~ px(140))
```

The reference line shows each country's mean population across the years, helping identify whether recent years are above or below the historical average. This horizontal line provides a quick visual benchmark: bars extending above the line represent years with above-average population, while those falling below indicate below-average years. The reference line is interactive. Positioning your mouse pointer to the right of the reference line reveals the computed mean value in a tooltip, formatted in the same way as the individual bar values. This allows you to see both the visual pattern and the precise threshold value that defines the comparison.

## Box plots

Box plots summarize distributions by showing median, quartiles, and outliers. They're ideal when each row contains many values and you want to convey distributional characteristics rather than individual data points.

### Basic box plots

```{r}
pizzaplace |>
  dplyr::filter(date <= "2015-01-14") |>
  dplyr::mutate(time_numeric = as.numeric(hms::as_hms(time))) |>
  dplyr::summarize(
    times = paste(time_numeric, collapse = ","),
    n_orders = dplyr::n(),
    .by = date
  ) |>
  gt() |>
  tab_header(title = "Pizza Order Timing", subtitle = "First Two Weeks of 2015") |>
  fmt_date(columns = date, date_style = "yMMMEd") |>
  cols_nanoplot(
    columns = times,
    plot_type = "boxplot",
    new_col_name = "timing",
    new_col_label = "Order Time Distribution"
  ) |>
  cols_width(timing ~ px(150)) |>
  cols_align(columns = timing, align = "center")
```

Each box plot summarizes that day's order timing distribution. The box spans the interquartile range (Q1 to Q3), the line inside marks the median, and whiskers extend to data within 1.5× IQR. Points beyond the whiskers are outliers.

### Customizing box plot appearance

```{r}
# Generate sample distribution data
set.seed(23)

dplyr::tibble(
  group = LETTERS[1:5],
  values = purrr::map_chr(1:5, ~ paste(round(rnorm(30, mean = .x * 10, sd = 5), 1),     collapse = ","))
) |>
  gt(rowname_col = "group") |>
  tab_header(title = "Distribution Comparison") |>
  cols_nanoplot(
    columns = values,
    plot_type = "boxplot",
    autoscale = TRUE,
    new_col_name = "dist",
    new_col_label = "Distribution"
  ) |>
  cols_width(dist ~ px(180))
```

The `autoscale = TRUE` option ensures all box plots share the same scale, making cross-row comparisons valid. Without this, each box plot would scale independently to its own data range.

### Formatting box plot hover values

Box plots can display custom-formatted values on hover:

```{r}
pizzaplace |>
  dplyr::filter(date <= "2015-01-07") |>
  dplyr::mutate(time_numeric = as.numeric(hms::as_hms(time))) |>
  dplyr::summarize(
    times = paste(time_numeric, collapse = ","),
    .by = date
  ) |>
  gt() |>
  tab_header(title = "Order Time Distributions") |>
  fmt_date(columns = date, date_style = "yMd") |>
  cols_nanoplot(
    columns = times,
    plot_type = "boxplot",
    new_col_name = "timing",
    new_col_label = "When Orders Came In",
    options = nanoplot_options(
      y_val_fmt_fn = function(x) format(hms::as_hms(x), "%H:%M")
    )
  ) |>
  cols_width(timing ~ px(160))
```

The `y_val_fmt_fn` argument accepts a function that transforms numeric values for display. Here, seconds-since-midnight values convert back to readable times when users hover over the plot.

## Reference lines and reference areas

Reference elements provide context by marking specific values or ranges within nanoplots.

### Reference lines

A reference line is a horizontal line marking a particular value. It can be a fixed number or computed from the data:

```{r}
illness |>
  dplyr::slice_head(n = 6) |>
  gt(rowname_col = "test") |>
  cols_hide(columns = c(starts_with("norm"), units)) |>
  cols_nanoplot(
    columns = starts_with("day"),
    reference_line = "median",
    new_col_name = "trend",
    new_col_label = "Trend (median reference)"
  ) |>
  cols_width(trend ~ px(120))
```

The reference line shows each test's median value across the seven days. Values above or below this baseline become immediately apparent.

Available keywords for computed reference lines:

- `"mean"`: arithmetic mean of the data
- `"median"`: median value
- `"min"`: minimum value
- `"max"`: maximum value
- `"q1"`: first quartile (25th percentile)
- `"q3"`: third quartile (75th percentile)
- `"first"`: first data value
- `"last"`: last data value

Or supply a fixed numeric value:

```{r}
towny |>
  dplyr::select(name, starts_with("population")) |>
  dplyr::slice_max(population_2021, n = 5) |>
  gt() |>
  fmt_integer(columns = starts_with("population")) |>
  cols_nanoplot(
    columns = starts_with("population"),
    reference_line = 500000,
    new_col_name = "trend",
    new_col_label = "Population History"
  ) |>
  cols_hide(columns = matches("199|200|201")) |>
  cols_width(trend ~ px(130))
```

The fixed 500,000 reference line provides a common benchmark across all municipalities.

### Reference areas

Reference areas shade a horizontal band, marking a range of values:

```{r}
illness |>
  dplyr::slice_head(n = 6) |>
  gt(rowname_col = "test") |>
  cols_hide(columns = c(starts_with("norm"), units)) |>
  cols_nanoplot(
    columns = starts_with("day"),
    reference_area = c("q1", "q3"),
    new_col_name = "trend",
    new_col_label = "Trend (IQR shaded)"
  ) |>
  cols_width(trend ~ px(130))
```

The shaded area marks the interquartile range. Values within this band represent "typical" observations while those outside are relatively extreme.

You can combine keywords and fixed values:

```{r}
towny |>
  dplyr::select(name, starts_with("density")) |>
  dplyr::slice_max(density_2021, n = 5) |>
  gt() |>
  fmt_number(columns = starts_with("density"), decimals = 0) |>
  cols_nanoplot(
    columns = starts_with("density"),
    reference_line = "median",
    reference_area = c("min", "max"),
    new_col_name = "trend",
    new_col_label = "Density Trend",
    options = nanoplot_options(
      reference_line_color = "darkred",
      reference_area_fill_color = "lightyellow"
    )
  ) |>
  cols_hide(columns = matches("199|200|201")) |>
  cols_width(trend ~ px(130))
```

The yellow area spans the full data range while the red line marks the median, showing both the overall scale and central tendency.

## Handling missing values

Real data often contains missing values. The `missing_vals` argument controls how nanoplots handle `NA`s:

### Gap strategy (default)

```{r}
dplyr::tibble(
  item = c("A", "B", "C"),
  v1 = c(10, 8, NA),
  v2 = c(15, NA, 12),
  v3 = c(NA, 14, 15),
  v4 = c(20, 16, 18),
  v5 = c(18, 12, NA)
) |>
  gt(rowname_col = "item") |>
  tab_header(title = "Missing Values: Gap Strategy") |>
  cols_nanoplot(
    columns = starts_with("v"),
    missing_vals = "gap",
    new_col_name = "trend",
    new_col_label = "Values"
  )
```

Gaps appear where data is missing. Lines discontinue and resume, clearly indicating where observations are absent.

### Marker strategy

The marker strategy works like the gap strategy but adds special visual markers at the locations of missing values. This draws even more attention to the fact that data is absent:

```{r}
dplyr::tibble(
  item = c("A", "B", "C"),
  v1 = c(10, 8, NA),
  v2 = c(15, NA, 12),
  v3 = c(NA, 14, 15),
  v4 = c(20, 16, 18),
  v5 = c(18, 12, NA)
) |>
  gt(rowname_col = "item") |>
  tab_header(title = "Missing Values: Marker Strategy") |>
  cols_nanoplot(
    columns = starts_with("v"),
    missing_vals = "marker",
    new_col_name = "trend",
    new_col_label = "Values"
  )
```

Like gaps, but with prominent markers at missing data locations. This makes missingness even more visible.

### Zero strategy

The zero strategy treats missing values as zeros. This approach is appropriate when absence of data genuinely means zero (for instance, missing order counts likely mean no orders occurred):

```{r}
dplyr::tibble(
  item = c("A", "B", "C"),
  v1 = c(10, 8, NA),
  v2 = c(15, NA, 12),
  v3 = c(NA, 14, 15),
  v4 = c(20, 16, 18),
  v5 = c(18, 12, NA)
) |>
  gt(rowname_col = "item") |>
  tab_header(title = "Missing Values: Zero Strategy") |>
  cols_nanoplot(
    columns = starts_with("v"),
    missing_vals = "zero",
    new_col_name = "trend",
    new_col_label = "Values"
  )
```

Missing values are replaced with zeros. Use this when zeros are meaningful substitutes (e.g., missing sales might truly mean zero sales).

### Remove strategy

The remove strategy excludes missing values entirely from the plot, connecting the remaining points directly. This can be useful when you want to focus only on observed values without calling attention to gaps:

```{r}
dplyr::tibble(
  item = c("A", "B", "C"),
  v1 = c(10, 8, NA),
  v2 = c(15, NA, 12),
  v3 = c(NA, 14, 15),
  v4 = c(20, 16, 18),
  v5 = c(18, 12, NA)
) |>
  gt(rowname_col = "item") |>
  tab_header(title = "Missing Values: Remove Strategy") |>
  cols_nanoplot(
    columns = starts_with("v"),
    missing_vals = "remove",
    new_col_name = "trend",
    new_col_label = "Values"
  )
```

Missing values are simply removed, and remaining values connect directly. The plots may have different numbers of points, but no gaps appear.

## Data input formats

Nanoplots accept data in two main formats: values spread across columns, or value streams in a single column.

### Multi-column format

The examples above primarily used multi-column format, where each column contains one value per row:

```{r}
exibble |>
  dplyr::select(row, num, currency) |>
  gt(rowname_col = "row") |>
  cols_nanoplot(
    columns = c(num, currency),
    new_col_name = "values",
    new_col_label = "Num & Currency"
  )
```

Values are collected left-to-right in the order columns are specified.

### Value stream format

While spreading values across multiple columns works well when your data is already structured that way, there are situations where packing values into a single delimited string offers significant advantages. This "value stream" format is useful when different rows contain varying numbers of data points, when data arrives from external sources in this format, or when you want to keep your table structure simple without creating many intermediate columns that serve only as nanoplot inputs.

Value streams are particularly useful when working with time series data of irregular length, aggregated measurements, or any scenario where the number of observations varies by row. Instead of dealing with missing values in unused columns or complex data reshaping, you can store each row's complete data series as a comma-separated (or space-separated, or semicolon-separated) string. **gt** parses these strings automatically, making it seamless to work with data in this format.

Value streams pack multiple values into a single column as delimited strings:

```{r}
dplyr::tibble(
  product = c("Widget", "Gadget", "Gizmo"),
  weekly_sales = c(
    "120, 135, 142, 128, 156, 149, 163",
    "85, 92, 88, 95, 101, 98, 105",
    "210, 198, 225, 232, 218, 245, 238"
  )
) |>
  gt(rowname_col = "product") |>
  tab_header(title = "Weekly Sales Trends") |>
  cols_nanoplot(
    columns = weekly_sales,
    new_col_name = "trend",
    new_col_label = "7-Day Trend"
  ) |>
  cols_width(trend ~ px(120))
```

Commas, spaces, or semicolons can separate values. Value streams are useful when different rows have varying numbers of observations, since each row's string can contain however many values exist for that row.

### Datetime value streams

Value streams can also contain ISO 8601 datetimes, which are automatically converted to numeric values:

```{r}
dplyr::tibble(
  event = c("Launch", "Update", "Sale"),
  timestamps = c(
    "2024-01-15 09:00:00, 2024-01-15 14:30:00, 2024-01-15 18:45:00",
    "2024-02-20 08:15:00, 2024-02-20 11:00:00, 2024-02-20 16:30:00, 2024-02-20 20:00:00",
    "2024-03-10 10:00:00, 2024-03-10 12:00:00, 2024-03-10 15:00:00"
  ),
  activity = c("50, 120, 85", "30, 75, 95, 60", "200, 350, 280")
) |>
  gt(rowname_col = "event") |>
  tab_header(title = "Activity Over Time") |>
  cols_nanoplot(
    columns = activity,
    columns_x_vals = timestamps,
    new_col_name = "timeline",
    new_col_label = "Activity Pattern"
  ) |>
  cols_width(timeline ~ px(140))
```

The datetime strings become x-axis positions, accurately representing the timing of observations.

## Autoscaling and axis control

By default, nanoplots scale independently: each plot adjusts its axis range to fit its own data. While this maximizes detail within each plot, it can mislead when comparing across rows. The `autoscale` option addresses this by forcing all nanoplots to share a common scale, and the `expand_x` and `expand_y` arguments allow you to set explicit axis ranges for even greater control.

### Autoscaling across rows

By default, each nanoplot scales independently to its own data range. This maximizes visual variation within each plot but makes cross-row comparison difficult:

```{r}
towny |>
  dplyr::select(name, starts_with("population")) |>
  dplyr::filter(population_2021 > 100000) |>
  dplyr::slice_head(n = 5) |>
  gt() |>
  fmt_integer(columns = starts_with("population")) |>
  cols_nanoplot(
    columns = starts_with("population"),
    autoscale = FALSE,  
    new_col_name = "trend",
    new_col_label = "Trend (independent scales)"
  ) |>
  cols_hide(columns = matches("199|200|201")) |>
  cols_width(trend ~ px(120))
```

Each municipality's plot fills its available space regardless of absolute population differences.

With `autoscale = TRUE`, all plots share the same y-axis scale:

```{r}
towny |>
  dplyr::select(name, starts_with("population")) |>
  dplyr::filter(population_2021 > 100000) |>
  dplyr::slice_head(n = 5) |>
  gt() |>
  fmt_integer(columns = starts_with("population")) |>
  cols_nanoplot(
    columns = starts_with("population"),
    autoscale = TRUE,  
    new_col_name = "trend",
    new_col_label = "Trend (common scale)"
  ) |>
  cols_hide(columns = matches("199|200|201")) |>
  cols_width(trend ~ px(120))
```

Now the visual heights accurately represent population magnitudes relative to other rows. Toronto's massive population dominates while smaller cities appear proportionally smaller.

### Expanding axis ranges

The `expand_x` and `expand_y` arguments extend plot boundaries beyond the data range:

```{r}
dplyr::tibble(
  scenario = c("Optimistic", "Baseline", "Pessimistic"),
  projections = c("105, 112, 120, 130", "100, 102, 105, 108", "100, 95, 88, 80")
) |>
  gt(rowname_col = "scenario") |>
  tab_header(title = "Revenue Projections (Indexed to 100)") |>
  cols_nanoplot(
    columns = projections,
    expand_y = c(70, 140),
    reference_line = 100,
    new_col_name = "projection",
    new_col_label = "4-Year Outlook"
  ) |>
  cols_width(projection ~ px(120))
```

The fixed y-axis range (`70`–`140`) and reference line at `100` provide consistent context across all scenarios, making it easy to see which projections exceed or fall below the baseline.

## Column positioning and labeling

When `cols_nanoplot()` creates a new column, you control where it appears and what it's called. The `before` and `after` arguments position the column relative to existing columns, while `new_col_name` and `new_col_label` set its internal name and display label.

### Positioning the nanoplot column

The `before` and `after` arguments control where the new nanoplot column appears:

```{r}
exibble |>
  dplyr::select(row, char, num, currency) |>
  gt(rowname_col = "row") |>
  cols_nanoplot(
    columns = c(num, currency),
    new_col_name = "values_plot",
    new_col_label = "Visual",
    after = "char"  
  )
```

The nanoplot column appears immediately after `char`, placing related information together.

### Custom column names and labels

The `new_col_name` and `new_col_label` arguments work together to give your nanoplot column both a practical internal identifier and a polished display label:

```{r}
illness |>
  dplyr::slice_head(n = 5) |>
  gt(rowname_col = "test") |>
  cols_hide(columns = c(starts_with("norm"), units)) |>
  cols_nanoplot(
    columns = starts_with("day"),
    new_col_name = "weekly_progression",
    new_col_label = md("*7-Day Progression*")
  )
```

The `new_col_name` sets the internal column name (useful for subsequent operations), while `new_col_label` sets the display label. Labels can include markdown formatting via `md()`.

## The `nanoplot_options()` helper

The `nanoplot_options()` function provides granular control over every visual aspect of nanoplots. Let's explore the major option categories:

**Function Signature**

```r
nanoplot_options(
  # Data point options
  data_point_radius = NULL,
  data_point_stroke_color = NULL,
  data_point_stroke_width = NULL,
  data_point_fill_color = NULL,
  
  # Data line options
  data_line_type = NULL,
  data_line_stroke_color = NULL,
  data_line_stroke_width = NULL,
  
  # Data area options
  data_area_fill_color = NULL,
  
  # Bar options (positive values)
  data_bar_stroke_color = NULL,
  data_bar_stroke_width = NULL,
  data_bar_fill_color = NULL,
  
  # Bar options (negative values)
  data_bar_negative_stroke_color = NULL,
  data_bar_negative_stroke_width = NULL,
  data_bar_negative_fill_color = NULL,
  
  # Reference elements
  reference_line_color = NULL,
  reference_area_fill_color = NULL,
  
  # Interactive guides
  vertical_guide_stroke_color = NULL,
  vertical_guide_stroke_width = NULL,
  
  # Layer visibility
  show_data_points = NULL,
  show_data_line = NULL,
  show_data_area = NULL,
  show_reference_line = NULL,
  show_reference_area = NULL,
  show_vertical_guides = NULL,
  show_y_axis_guide = NULL,
  
  # Value formatting
  interactive_data_values = NULL,
  y_val_fmt_fn = NULL,
  y_axis_fmt_fn = NULL,
  y_ref_line_fmt_fn = NULL,
  currency = NULL
)
```

### Per-point styling

Some options accept vectors to style individual data points differently:

```{r}
dplyr::tibble(
    quarter = c("Q1", "Q2", "Q3", "Q4"),
    revenue = c("100, 110, 105, 120")
) |>
    gt(rowname_col = "quarter") |>
    cols_nanoplot(
        columns = revenue,
        new_col_name = "trend",
        options = nanoplot_options(
            data_point_fill_color = c("gray", "gray", "gray", "gold"),
            data_point_radius = c(5, 5, 5, 10),
            data_point_stroke_color = "black",
            data_line_stroke_color = "gray"
        )
    )
```

The final point (Q4) is highlighted with a larger golden marker (look at the final values in the `data_point_fill_color` and `data_point_radius` arguments), drawing attention to the most recent value.

### Reusable option sets

When creating tables with multiple nanoplot columns, you often want them to share a consistent visual style. Rather than duplicating the same `nanoplot_options()` specification for each column, you can define an option set once and reuse it. This approach offers several benefits: it reduces code repetition, ensures perfect visual consistency across columns, and makes style updates trivial (change the definition once rather than hunting through multiple `cols_nanoplot()` calls).

Reusable option sets are particularly valuable when building themed tables or when working with organizational style guidelines. You might define several standard option sets (`"minimal"`, `"detailed"`, `"dashboard"`, etc.) and apply them consistently across different tables and reports. This creates visual coherence across your work while keeping the implementation clean and maintainable.

Create option sets once and apply them across multiple nanoplot columns:

```{r}
minimal_style <- nanoplot_options(
  show_data_area = FALSE,
  show_data_points = FALSE,
  data_line_stroke_width = px(2),
  data_line_stroke_color = "#333333",
  data_line_type = "straight"
)

towny |>
  dplyr::select(name, starts_with("population"), starts_with("density")) |>
  dplyr::slice_max(population_2021, n = 4) |>
  gt() |>
  cols_nanoplot(
    columns = starts_with("population"),
    new_col_name = "pop_trend",
    new_col_label = "Population",
    options = minimal_style
  ) |>
  cols_nanoplot(
    columns = starts_with("density"),
    new_col_name = "dens_trend",
    new_col_label = "Density",
    options = minimal_style
  ) |>
  cols_hide(columns = -c(name, pop_trend, dens_trend))
```

Both nanoplot columns share the same minimal styling, creating visual consistency across the table.

### Currency formatting

When nanoplots display financial data, proper currency formatting in tooltips and hover displays makes the values immediately understandable. Rather than seeing raw numbers like "1350" or "12000", users see properly formatted currency values like `"$1,350"` or `"$12,000"`. This formatting applies to the interactive elements of the nanoplot: when users hover over data points, reference lines, or other interactive features, the displayed values respect the currency specification.

The `currency` option in `nanoplot_options()` accepts standard three-letter currency codes (`"USD"`, `"EUR"`, `"GBP"`, `"JPY"`, etc.) and automatically applies appropriate formatting rules for that currency, including the correct symbol, decimal places, and thousands separators. This ensures that financial nanoplots maintain the same level of polish and professionalism as the rest of your formatted table columns.

For financial data, specify a currency code:

```{r}
dplyr::tibble(
  product = c("Basic", "Pro", "Enterprise"),
  monthly_revenue = c(
    "1200, 1350, 1280, 1420, 1510",
    "4500, 4800, 5100, 4950, 5300",
    "12000, 13500, 14200, 15800, 16500"
  )
) |>
  gt(rowname_col = "product") |>
  tab_header(title = "Monthly Revenue Trends") |>
  cols_nanoplot(
    columns = monthly_revenue,
    reference_line = "mean",
    new_col_name = "trend",
    new_col_label = "5-Month Trend",
    options = nanoplot_options(currency = "USD")
  ) |>
  cols_width(trend ~ px(140))
```

Hovering over data points displays values formatted as currency (e.g., `"$1,350"` instead of `"1350"`).

## Practical examples

Nanoplots shine when they combine multiple techniques like autoscaling for valid comparisons, reference elements for context, custom styling for clarity, and meticulous positioning for narrative flow. These examples demonstrate complete workflows that bring together the concepts covered in this chapter.

### Sparkline summary table

This example combines monthly summary statistics with daily price trend sparklines, providing both high-level metrics and visual context:

```{r}
sp500 |>
  dplyr::filter(date >= "2015-01-01" & date <= "2015-12-31") |>
  dplyr::mutate(month = format(date, "%B")) |>
  dplyr::mutate(month = factor(month, levels = month.name)) |>
  dplyr::summarize(
    open = first(open),
    close = last(close),
    high = max(high),
    low = min(low),
    prices = paste(close, collapse = ","),
    .by = month
  ) |>
  dplyr::arrange(month) |>
  gt(rowname_col = "month") |>
  tab_header(
    title = "S&P 500 Monthly Summary",
    subtitle = "2015 Calendar Year"
  ) |>
  fmt_currency(columns = c(open, close, high, low), decimals = 0) |>
  cols_nanoplot(
    columns = prices,
    new_col_name = "daily_trend",
    new_col_label = "Daily Closes",
    options = nanoplot_options(
      show_data_points = FALSE,
      show_data_area = FALSE,
      data_line_stroke_color = "steelblue",
      data_line_stroke_width = 1.5
    )
  ) |>
  cols_width(daily_trend ~ px(100)) |>
  cols_move(columns = daily_trend, after = open)
```

This financial summary table combines key statistics with a visual representation of daily price movements. The sparkline provides trend context that complements the summary figures.

### Distribution comparison table

Box plot nanoplots are great at comparing distributions across groups, revealing differences in central tendency, spread, and outliers:

```{r}
set.seed(23)

dplyr::tibble(
  treatment = c("Control", "Drug A", "Drug B", "Drug C"),
  responses = purrr::map_chr(
    c(50, 55, 48, 62),
    ~ paste(round(rnorm(25, mean = .x, sd = 10), 1), collapse = ",")
  )
) |>
  gt(rowname_col = "treatment") |>
  tab_header(title = "Treatment Response Distributions") |>
  cols_nanoplot(
    columns = responses,
    plot_type = "boxplot",
    autoscale = TRUE,
    new_col_name = "distribution",
    new_col_label = "Response Distribution"
  ) |>
  cols_width(distribution ~ px(200))
```

Box plots reveal distributional differences between treatments (not just central tendency but spread and outliers). The shared scale (via `autoscale`) enables valid visual comparison.

### Multi-metric dashboard row

This dashboard-style table demonstrates how multiple nanoplot columns with different plot types and color schemes can work together to tell a comprehensive story:

```{r}
# Simulated performance metrics
dplyr::tibble(
    server = c("prod-1", "prod-2", "prod-3"),
    cpu = c("45,52,48,61,55,49,53", "78,82,79,85,81,77,80", "32,35,38,34,36,33,37"),
    memory = c("62,65,64,68,66,63,67", "71,74,72,76,73,70,75", "55,58,56,60,57,54,59"),
    requests = c(
        "1200,1350,1280,1420,1380,1250,1310",
        "890,920,905,940,915,885,910",
        "1580,1620,1595,1650,1610,1570,1605"
    )
) |>
    gt(rowname_col = "server") |>
    tab_header(title = "Server Performance (Last 7 Hours)") |>
    cols_nanoplot(
        columns = cpu,
        autoscale = TRUE,
        new_col_name = "cpu_plot",
        new_col_label = "CPU %",
        options = nanoplot_options(
            show_data_area = FALSE,
            data_line_stroke_color = "#E74C3C",
            data_point_fill_color = "#E74C3C"
        )
    ) |>
    cols_nanoplot(
        columns = memory,
        autoscale = TRUE,
        new_col_name = "mem_plot",
        new_col_label = "Memory %",
        options = nanoplot_options(
            show_data_area = FALSE,
            data_line_stroke_color = "#3498DB",
            data_point_fill_color = "#3498DB"
        )
    ) |>
    cols_nanoplot(
        columns = requests,
        plot_type = "bar",
        autoscale = TRUE,
        new_col_name = "req_plot",
        new_col_label = "Requests/hr",
        options = nanoplot_options(
            data_bar_fill_color = "#2ECC71",
            data_bar_stroke_color = "#27AE60"
        )
    ) |>
    cols_width(
        c(cpu_plot, mem_plot) ~ px(100),
        req_plot ~ px(80)
    )
```

This dashboard-style table uses color-coded nanoplots to show multiple metrics per server. Line plots suit the percentage metrics while bar plots work well for discrete request counts.

Nanoplots transform tables from static data presentations into dynamic visual summaries. By embedding trend lines, distributions, and comparisons directly within table rows, they enable readers to grasp patterns at a glance while retaining access to precise numeric values. The extensive customization options ensure that nanoplots can be tailored to match any design aesthetic or analytical purpose.

## Summary

This chapter has explored nanoplots: compact visualizations that embed directly within table cells, bridging the gap between tabular precision and visual pattern recognition.

The key capabilities we've covered:

- plot types: line plots show trends over time or sequence, bar plots display discrete comparisons, and box plots summarize distributions. Each type serves different analytical purposes.
- the `cols_nanoplot()` function creates a new column of visualizations from numeric data in existing columns. It handles data collection, plot generation, and column placement automatically.
- data formats: nanoplots accept data as separate columns, comma-separated strings within cells, or explicit x-y value pairs. This flexibility accommodates various data structures.
- reference elements: reference lines and reference areas add context to plots, showing targets, thresholds, or acceptable ranges that help readers interpret the visualizations.
- customization: the `nanoplot_options()` helper provides extensive control over colors, sizes, strokes, and display elements. You can match nanoplots to your table's overall design aesthetic.
- missing data handling: the `missing_vals` argument controls how gaps appear in plots (as breaks, markers, zeros, or removed points).
- autoscaling: when comparing across rows, `autoscale = TRUE` ensures all plots share the same axis ranges, making visual comparisons meaningful.

Nanoplots work best when they complement rather than replace numeric values. A trend line next to quarterly figures helps readers see the trajectory. A distribution box plot alongside summary statistics reveals shape. The combination of numbers and graphics creates tables that inform at multiple levels of detail.

The next chapter introduces table groups, which let you work with multiple related tables as a cohesive unit. You'll learn to bundle tables together, apply common styling, and output them as coordinated sets.
