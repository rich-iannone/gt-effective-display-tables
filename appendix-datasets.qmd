# The gt datasets {#sec-appendix-datasets}

```{r setup, include=FALSE, echo=FALSE}
library(gt)
library(dplyr)
```

Every dataset tells a story. The eighteen datasets bundled with **gt** were not assembled arbitrarily or pulled from dusty archives of convenient CSV files. Each one emerged from a specific need, a personal curiosity, or a gap in what was publicly available. Some model human behavior with surprising fidelity. Others preserve scientific data that would otherwise remain scattered across obscure government pages. A few exist simply because I thought "I wish this dataset existed" and then made it so.

This appendix takes you behind the scenes of these datasets. You will learn where they came from, how they were constructed, and why they matter beyond their immediate utility as demonstration data. Along the way, we will explore the broader contexts they represent: the economics of a neighborhood pizza shop, the 125-year evolution of urban transit in Paris, the delicate chemistry of Earth's atmosphere, and the quiet dramas of population change in Ontario's small towns. The datasets are more than rows and columns. They are windows into worlds worth understanding.

## pizzaplace

The `pizzaplace` dataset contains 49,574 rows representing every pizza sold at a fictional pizzeria during the year 2015. Each row records a transaction with its timestamp, pizza type, size, and price. On the surface it appears to be straightforward sales data. Underneath, it is an elaborate simulation of human behavior, kitchen operations, and the unpredictable rhythms of a small food business.

The inspiration for this dataset came from Plateau Pizza, a real establishment in Coquitlam, British Columbia. The restaurant occupies a pleasant spot in a suburban plaza alongside a dollar store called Dollars & Cents and an IGA grocery store. It is the kind of neighborhood pizza place that survives on regulars and convenience, offering pizzas with names that are equal parts cheesy and memorable. Goat Supreme. The Calabrese. Names that stick in your head even if you cannot quite remember what toppings they included.

The dataset borrowed liberally from Plateau Pizza's menu. Their category structure (Classic, Supreme, Veggie and Vegan, Chicken) was adopted directly. Many pizza names and ingredient combinations came straight from their website, though some were embellished and others invented entirely. The fictional additions drew inspiration from Food Network recipes and local salumist offerings that seemed appropriately gourmet. Pricing followed a similar pattern: real prices served as the baseline, then adjustments were made based on perceived ingredient costs. The fancier cheeses and cured meats commanded premium prices, as they should.

What makes `pizzaplace` genuinely interesting is not the menu but the simulation that generated a year's worth of orders. The modeling script (preserved in the gt package's source repository at `data-raw/05-pizzaplace.R`) creates synthetic customers who arrive throughout each day with realistic patterns. Barret Schloerke contributed refinements to the behavioral model, adding nuance to timing and preference distributions. Weekends see more traffic than weekdays, reflecting the work-leisure split that governs so much of consumer behavior. Holidays disrupt normal patterns in expected ways.

The simulation also includes what might be called narrative events. A kitchen fire at one point disrupts operations. These partially catastrophic incidents add verisimilitude to what would otherwise be suspiciously smooth data. Real businesses experience interruptions, equipment failures, staff shortages, and the occasional minor disaster. The `pizzaplace` data reflects this messiness, making it more valuable for realistic analysis exercises than perfectly clean synthetic data would be.

The dataset originally included plans for appetizers, side dishes, and beverages, but these were ultimately cut in favor of simplicity. A pizzeria that only sells pizza is easier to understand and analyze than one with a full menu. The constraint also keeps the focus on what makes the dataset distinctive: the careful modeling of how people order pizza throughout a year.

### The pizzas of pizzaplace

```{r}
#| echo: false
pizzaplace |>
  dplyr::distinct(type, name, size, price) |>
  dplyr::group_by(type, name) |>
  dplyr::summarize(
    sizes = paste(size, collapse = ", "),
    price_range = paste0(
      "$", sprintf("%.2f", min(price)), 
      " - $", sprintf("%.2f", max(price))
    ),
    .groups = "drop"
  ) |>
  dplyr::arrange(type, name) |>
  gt(groupname_col = "type") |>
  cols_label(
    name = "Pizza",
    sizes = "Available Sizes",
    price_range = "Price Range"
  ) |>
  tab_header(
    title = "The Complete pizzaplace Menu",
    subtitle = "All 32 pizza varieties by category"
  )
```

The menu reveals the character of the fictional establishment. Classic pizzas stick to familiar territory: pepperoni, Hawaiian, the combinations everyone recognizes. The Supreme category ventures into more elaborate ingredient lists with names that promise indulgence. Veggie options cater to those avoiding meat, while the Chicken category builds meals around that particular protein. Each pizza comes in multiple sizes, though not every pizza is available in every size. The XL (and XXL!) option appears only for a certain pizza, and presumably that one pizza is popular enough to warrant the larger formats (could be that the ingredient mix works well at scale).

Pricing follows intuitive logic. A basic cheese pizza costs less than one loaded with prosciutto and artichoke hearts. Size increases bring proportional price increases, though the per-square-inch cost typically decreases as you go larger (the eternal economic argument for ordering the bigger pizza). These pricing patterns create natural opportunities for analysis: which pizzas generate the most revenue? Which sizes sell best and for which types? How does day of week affect the popularity of the vegetarian options?

### A pizza tier list

Any discussion of pizza invites opinions about which pizzas are best. The following tier list represents one possible ranking of the `pizzaplace` menu, from essential classics to more adventurous options that may not suit every palate. Reasonable people will disagree, and that disagreement is part of what makes pizza culture endlessly entertaining.

```{r}
#| echo: false

pizza_ingredients <- dplyr::tribble(
  ~name, ~ingredients, ~tier, ~notes,
  "The Pepperoni", "Mozzarella, Pepperoni, Tomato Sauce", "S", "The benchmark against which all pizzas are measured",
  "The Big Meat", "Bacon, Pepperoni, Italian Sausage, Chorizo, Mozzarella, Tomato Sauce", "S", "Maximalist approach that somehow works",
  "The Classic Deluxe", "Pepperoni, Mushrooms, Red Onions, Red Peppers, Bacon, Mozzarella, Tomato Sauce", "S", "Everything a pizza should be",
  "The Barbecue Chicken", "Barbecued Chicken, Red Peppers, Green Peppers, Tomatoes, Red Onions, Mozzarella, Barbecue Sauce", "A", "Sweet and savory in perfect balance",
  "The Hawaiian", "Sliced Ham, Pineapple, Mozzarella, Tomato Sauce", "A", "Controversial but undeniably popular",
  "The Italian Capocollo", "Capocollo, Red Peppers, Tomatoes, Goat Cheese, Garlic, Oregano, Mozzarella, Tomato Sauce", "A", "Elevated ingredients lift the whole experience",
  "The Calabrese", "Nduja, Italian Sausage, Pepperoni, Tomatoes, Red Onions, Mozzarella, Tomato Sauce", "A", "Spicy nduja provides serious depth",
  "The Prosciutto", "Prosciutto, Arugula, Mozzarella, Tomato Sauce", "A", "Simple elegance, fresh and light",
  "The Four Cheese", "Ricotta, Gorgonzola, Romano, Mozzarella, Tomato Sauce", "B", "For dedicated cheese enthusiasts only",
  "The Vegetables", "Mushrooms, Tomatoes, Red Peppers, Green Peppers, Red Onions, Zucchini, Spinach, Garlic, Mozzarella, Tomato Sauce", "B", "The vegetable abundance can overwhelm",
  "The Spinach Pesto", "Spinach, Artichokes, Tomatoes, Sun-dried Tomatoes, Garlic, Pesto Sauce, Mozzarella", "B", "Pesto base divides opinion",
  "The Greek", "Kalamata Olives, Feta, Tomatoes, Red Onions, Red Peppers, Garlic, Mozzarella, Tomato Sauce", "B", "Mediterranean flavors work better as salad",
  "The Brie Carre", "Brie, Prosciutto, Caramelized Onions, Pears, Thyme, Garlic, Mozzarella", "C", "Ambitious but confused identity",
  "The Chicken Pesto", "Chicken, Tomatoes, Red Peppers, Spinach, Garlic, Pesto Sauce, Mozzarella", "C", "Pesto and chicken compete rather than complement",
  "The Soppressata", "Soppressata, Fontina, Mozzarella, Garlic, Tomato Sauce", "C", "Underwhelming given premium ingredients"
)

pizza_ingredients |>
  gt(groupname_col = "tier") |>
  cols_label(
    name = "Pizza",
    ingredients = "Ingredients",
    notes = "Assessment"
  ) |>
  tab_row_group(label = md("**S Tier** — Essential"), rows = tier == "S") |>
  tab_row_group(label = md("**A Tier** — Excellent"), rows = tier == "A") |>
  tab_row_group(label = md("**B Tier** — Good"), rows = tier == "B") |>
  tab_row_group(label = md("**C Tier** — Passable"), rows = tier == "C") |>
  row_group_order(groups = c(
    md("**S Tier** — Essential"),
    md("**A Tier** — Excellent"),
    md("**B Tier** — Good"),
    md("**C Tier** — Passable")
  )) |>
  tab_header(
    title = "The Definitive pizzaplace Tier List",
    subtitle = "A highly subjective ranking of 15 notable pizzas"
  ) |>
  tab_source_note("Rankings reflect one person's taste. YMMV.")
```

The S-tier pizzas represent the core of what a pizzeria should do well. The Pepperoni is the foundation, the pizza against which all others are implicitly compared. When someone says "let's get pizza", this is what most people imagine. The Big Meat takes maximalism seriously but avoids the trap of incoherence: bacon, pepperoni, sausage, and chorizo sound excessive, but they harmonize around a common meatiness (it really works!). The Classic Deluxe achieves balance, incorporating vegetables (mushrooms, peppers, onions) alongside meat without letting any single ingredient dominate.

The A-tier pizzas represent successful experiments. The Hawaiian remains perpetually controversial, inspiring passionate defenses and equally passionate condemnations. But I'm a fan. The combination of salty ham and sweet pineapple against tangy tomato sauce works for a lot of people, and the dataset shows it selling consistently throughout the year. The Italian Capocollo and Calabrese elevate the genre through premium cured meats, offering pizzas that could credibly appear on a trattoria menu. The Prosciutto demonstrates that restraint can be a virtue: just ham, arugula, and cheese, but executed with quality ingredients.

The B-tier pizzas are solid but not essential. The Four Cheese appeals to dedicated turophiles but can feel monotonous without the textural variety that vegetables or meats provide. The vegetable-heavy options (Vegetables, Greek, Spinach Pesto) often release too much moisture during cooking, resulting in soggy centers that undermine the crust. Don't get me wrong, they are fine pizzas but rarely anyone's first choice.

The C-tier pizzas represent experiments that did not quite succeed. The Brie Carre attempts a French-inspired combination of brie, pears, and prosciutto that sounds sophisticated but tastes confused. The ingredients come from different culinary traditions and never fully integrate. These pizzas *do* seem to sell in this particular dataset, but they probably rarely inspire repeat orders or enthusiastic recommendations.

### What makes a good pizza

The tier list above reflects accumulated pizza wisdom, but the principles underlying it deserve articulation. A good pizza balances several competing demands. The crust must be sturdy enough to support toppings without becoming soggy, yet thin and pliable enough to fold. The sauce should be present but not overwhelming, providing acidity and moisture without drowning the other flavors. The cheese needs to melt smoothly and brown slightly without becoming rubbery or releasing pools of grease. The toppings must be distributed evenly and cut to appropriate sizes so that each bite contains a representative sample. And freshness is non-negotiable. A pizza that has been languishing in a display case for hours, slowly drying out under a heat lamp, is a shadow of its former self. The crust toughens, the cheese congeals, and the toppings develop that dispiriting sheen of oxidation. At the extreme end of neglect, [one reviewer actually found mold growing on the underside of a display pizza](https://www.youtube.com/shorts/opoizv5lH0c), which is the sort of discovery that makes you reconsider every grab-and-go slice you have ever eaten.

Beyond these structural requirements, good pizza demonstrates restraint. The impulse to add more toppings is understandable but usually misguided. Each additional ingredient dilutes the impact of everything else. The best pizzas feature three to five components (beyond sauce and cheese) that complement one another. Pepperoni alone is better than pepperoni with six other meats. Mushrooms and peppers work together because their textures contrast and their flavors do not compete.

Temperature matters enormously. A pizza that has been sitting for twenty minutes bears little resemblance to one fresh from the oven. The cheese firms up, the crust softens, the toppings cool into sad disconnection. This is why pizzerias survive on dine-in and delivery rather than takeout that sits in a car. And do not even get me started on microwave pizza. The microwave does something deeply wrong to pizza crust, transforming it into a chewy, rubbery substance that no longer qualifies as bread by any reasonable definition. The cheese melts unevenly, the sauce superheats into lava pockets, and the whole experience leaves you wondering why you bothered. Cold leftover pizza eaten standing at the refrigerator at midnight is honestly preferable (I've actually grown to like it more and more these days). The pizzaplace dataset implicitly captures this reality: the simulation models customers who order and receive pizzas within a reasonable timeframe, not pizzas boxed and forgotten (and certainly not pizzas reheated in a microwave).

Finally, good pizza requires good ingredients. This seems obvious but explains why some inexpensive pizzas disappoint while others satisfy. The quality of the mozzarella matters. The tomatoes in the sauce matter. Even the olive oil brushed on the crust matters. Plateau Pizza, the real establishment that inspired the dataset, succeeds partly because it sources decent ingredients and does not cut corners that customers would notice. The fictional pizzaplace inherits this philosophy.

And yet, after all this talk of balance and restraint, it is worth acknowledging that pizza can abandon every one of these conventions and still be legitimate. A pizza can have no sauce. It can have no cheese. It can be nothing more than dough, olive oil, and anchovies. This sounds shocking to anyone raised on North American delivery pizza, but it is a perfectly valid expression of pizzadom with deep roots in Italian tradition. Pizza bianca, pizza marinara, and the various focaccia-adjacent flatbreads of southern Italy predate the mozzarella-laden versions by centuries. The rules above describe one tradition. Pizza is generous enough to accommodate others.

### Why pizza data matters

Pizza occupies a unique position in the landscape of consumer goods. It is simultaneously simple (bread, sauce, cheese, toppings) and infinitely variable. A pizzeria's menu encodes assumptions about its customers: their adventurousness, their price sensitivity, their dietary restrictions. Sales patterns encode behavior: when people eat, what they celebrate, how weather affects appetite.

Part of my motivation for creating this dataset was to draw attention to pizza analytics as a legitimate field of inquiry. The phrase sounds ridiculous, and that is partly the point. If a dataset can make someone smile while also teaching them time series decomposition and category performance analysis, it has done more work than a dataset that only accomplishes the second thing. Nobody has ever felt intimidated by pizza data. Nobody has ever opened a CSV of pizza orders and thought "I am not qualified to analyze this". The approachability is a feature (not a frivolity!).

The `pizzaplace` dataset serves as a sandbox for the kinds of analysis that businesses perform constantly. Revenue breakdowns, time series decomposition, category performance, seasonal adjustment. These techniques apply far beyond pizza. Anyone learning to analyze transactional data will find the patterns in `pizzaplace` transferable to retail, hospitality, and service industries generally. The dataset is large enough to be realistic (nearly 50,000 transactions) but small enough to process quickly on any modern computer.

For **gt** specifically, `pizzaplace` demonstrates grouped data, aggregation, currency formatting, and the presentation of time-based information. A year of pizza sales can become a monthly summary table, a daily heatmap, a ranked list of bestsellers, or a comparison across categories. The richness of the underlying data supports dozens of different table designs.

## `exibble`

The name `exibble` is a portmanteau of "example tibble" and it serves exactly that purpose. This tiny dataset of eight rows exists to demonstrate **gt**'s formatting capabilities without the distraction of meaningful content. Each column represents a different data type: numeric values, character strings, currency amounts, dates, times, datetimes, and logical values. Missing values appear in strategic locations to demonstrate `sub_missing()` and related substitution functions.

```{r}
gt(exibble)
```

The column names are deliberately generic (`num`, `char`, `currency`, `date`, `time`, `datetime`) because the content does not matter. What matters is having every common data type available in a single compact dataset. When documenting a date formatter, you need a date column. When showing number formatting options, you need numbers. When explaining how to handle NA values, you need NA values in predictable locations.

### Anatomy of a reference dataset

Each row and column in `exibble` was chosen to exercise different aspects of table formatting:

```{r}
#| echo: false
tibble::tibble(
  Column = c("num", "char", "currency", "date", "time", "datetime", "row", "group"),
  Type = c("numeric", "character", "numeric", "Date", "character", "POSIXct", "character", "character"),
  Purpose = c(
    "Demonstrates numeric formatting across scales",
    "Provides recognizable text labels (fruits)",
    "Tests currency with decimals, zeros, NAs",
    "Shows date formatting patterns",
    "Character-encoded times for parsing",
    "Full datetime objects for formatting",
    "Stub/rowname labels for tables",
    "Group categories for row grouping"
  )
) |>
  gt(rowname_col = "Column") |>
  cols_label(
    Type = "R Type",
    Purpose = "Role in Examples"
  ) |>
  tab_header(title = "exibble Column Structure")
```

The fruit names in the `char` column (apricot, banana, coconut, and so forth) follow alphabetical order, which makes them easy to verify when demonstrating sorting or filtering operations. The numeric values span several orders of magnitude, from fractions to millions, ensuring that formatters must handle both small precise values and large rounded ones. The currency column includes a missing value and one very small amount, testing edge cases that might trip up naive formatting approaches.

The `row` and `group` columns transform `exibble` from a formatting showcase into a structural one. With `row` serving as a stub and `group` organizing rows into categories, the same eight-row dataset can demonstrate virtually every **gt** feature. Headers, stubs, row groups, column formatting, substitution, styling... all can be shown using just `exibble`.

```{r}
#| echo: false
exibble |>
  gt(rowname_col = "row", groupname_col = "group") |>
  fmt_number(columns = num, decimals = 2) |>
  fmt_currency(columns = currency, decimals = 2) |>
  fmt_date(columns = date, date_style = "yMd") |>
  fmt_time(columns = time, time_style = "Hm") |>
  fmt_datetime(columns = datetime, date_style = "yMd", time_style = "Hm") |>
  sub_missing(missing_text = "—") |>
  tab_header(
    title = "exibble with Row Groups and Stub",
    subtitle = "Demonstrating structural features"
  )
```

Datasets like `exibble` just don't get a lot of attention, however, they are essential as infrastructure for examples in documentation. Every example in **gt**'s documentation that needs to show a quick formatting demonstration reaches for `exibble` rather than constructing throwaway data inline. This consistency helps readers recognize the dataset and focus on what is being demonstrated rather than puzzling over unfamiliar data structures.

## `gtcars`

The `gtcars` dataset contains specifications for 47 luxury and performance automobiles, with an emphasis on grand touring vehicles. The name works on two levels: these are GT (grand tourer) cars, and the dataset lives in a package called **gt**. The wordplay is intentional but understated. I try not to make a big deal about it.

```{r}
gtcars |>
  dplyr::select(mfr, model, year, hp, trq, mpg_c, mpg_h, msrp) |>
  dplyr::slice_head(n = 10) |>
  gt(rowname_col = "model", groupname_col = "mfr") |>
  fmt_integer(columns = c(hp, trq)) |>
  fmt_currency(columns = msrp, decimals = 0) |>
  cols_label(
    year = "Year",
    hp = "HP",
    trq = "Torque",
    mpg_c = "MPG (city)",
    mpg_h = "MPG (hwy)",
    msrp = "MSRP"
  )
```

The dataset was assembled from Motor Trend articles about grand touring vehicles, with additional research filling in gaps for fuel economy and torque figures. Most vehicles date from around 2015, reflecting when the source articles were published. The selection criteria emphasized true grand tourers: vehicles designed for high-speed, long-distance driving in comfort, typically with powerful engines, refined interiors, and substantial price tags.

### What makes a grand tourer?

The grand touring concept originated in 1950s Europe, when wealthy motorists began taking extended driving holidays across the continent. A proper GT needed range (400+ kilometers between fuel stops), performance (for the autobahns and mountain passes), and comfort (for hours behind the wheel). The Ferrari 250 GT established the template: front-mounted V12, leather interior, elegant coachwork by Pininfarina or Scaglietti. The grand tourer was always as much about aspiration as transportation.

```{r}
#| echo: false
gtcars |>
  dplyr::group_by(mfr) |>
  dplyr::summarize(
    models = n(),
    avg_hp = mean(hp),
    avg_msrp = mean(msrp),
    avg_mpg = mean((mpg_c + mpg_h) / 2),
    .groups = "drop"
  ) |>
  dplyr::arrange(desc(avg_msrp)) |>
  head(10) |>
  gt(rowname_col = "mfr") |>
  fmt_integer(columns = c(models, avg_hp)) |>
  fmt_currency(columns = avg_msrp, decimals = 0) |>
  fmt_number(columns = avg_mpg, decimals = 1) |>
  cols_label(
    models = "Models",
    avg_hp = "Avg HP",
    avg_msrp = "Avg Price",
    avg_mpg = "Avg MPG"
  ) |>
  tab_header(
    title = "Manufacturers by Average Price",
    subtitle = "From the gtcars dataset"
  )
```

Ferrari commands the highest average prices in the dataset, followed by the other Italian marques and the boutique British manufacturers. The German manufacturers (BMW, Mercedes, Audi) cluster at somewhat lower price points but offer more models, representing a broader range from accessible sports cars to top-tier performance vehicles. American manufacturers appear in the dataset primarily through Ford (the GT supercar) and Chevrolet (the Corvette), carrying on a different tradition of performance cars with a more democratic price structure.

```{r}
#| echo: false
gtcars |>
  dplyr::filter(!is.na(hp) & !is.na(trq)) |>
  dplyr::mutate(power_tier = case_when(
    hp >= 600 ~ "Extreme (600+ HP)",
    hp >= 500 ~ "High (500-599 HP)",
    hp >= 400 ~ "Strong (400-499 HP)",
    TRUE ~ "Modest (<400 HP)"
  )) |>
  dplyr::group_by(power_tier) |>
  dplyr::summarize(
    count = n(),
    avg_torque = mean(trq),
    avg_price = mean(msrp),
    .groups = "drop"
  ) |>
  dplyr::mutate(
    power_tier = factor(
      power_tier,
      levels = c("Modest (<400 HP)", "Strong (400-499 HP)", "High (500-599 HP)", "Extreme (600+ HP)")
    )
  ) |>
  dplyr::arrange(power_tier) |>
  gt(rowname_col = "power_tier") |>
  fmt_integer(columns = c(count, avg_torque)) |>
  fmt_currency(columns = avg_price, decimals = 0) |>
  cols_label(
    count = "Models",
    avg_torque = "Avg Torque (lb-ft)",
    avg_price = "Avg Price"
  ) |>
  tab_header(title = "Performance Tiers")
```

The relationship between horsepower and price is neither linear nor deterministic. Some modest-horsepower vehicles (certain Porsches, for instance) command premium prices through brand cachet and driving dynamics. Some extremely powerful vehicles achieve their output through brute displacement rather than exotic engineering, keeping prices relatively accessible. The correlation exists, but the exceptions tell interesting stories.

The choice to create `gtcars` was motivated by a desire for a modern equivalent to the venerable `mtcars` dataset that has shipped with R for decades. The original `mtcars` contains 1974 Motor Trend data on 32 automobiles, and it has been used in countless examples and tutorials. But cars from 1974 feel increasingly remote from contemporary experience. A dataset of modern luxury vehicles offers familiar reference points (Ferrari, Porsche, Aston Martin) and specifications that relate to cars people actually see on roads today.

For table-making purposes, `gtcars` provides natural groupings by manufacturer, multiple numeric columns suitable for formatting and comparison, and a mix of discrete and continuous variables. The manufacturer and model columns enable row grouping and stub labeling. The price column practically demands currency formatting. The horsepower and torque columns work well for bar chart visualizations within cells. It is a dataset that seems designed for beautiful tables because, in fact, it was.

## `countrypops`

The `countrypops` dataset tracks population estimates for countries worldwide from 1960 through the present (currently extending to 2024). The data comes from the World Bank, which compiles demographic estimates from national statistical offices, census data, and the United Nations Population Division. With over 13,000 rows covering more than 200 countries across six decades, it is one of the larger datasets in the **gt** collection.

```{r}
countrypops |>
  dplyr::filter(country_code_3 %in% c("USA", "CHN", "IND", "BRA", "NGA")) |>
  dplyr::filter(year %in% c(1960, 1980, 2000, 2020)) |>
  dplyr::select(country_name, year, population) |>
  tidyr::pivot_wider(names_from = year, values_from = population) |>
  gt(rowname_col = "country_name") |>
  fmt_number(suffixing = TRUE) |>
  tab_header(title = "Population Growth in Five Major Nations")
```

Population data might seem straightforward, but it encodes profound stories of human migration, economic development, public health, and political change. China's population trajectory shows the demographic impact of the one-child policy. Nigeria's explosive growth reflects patterns common across sub-Saharan Africa. European countries exhibit the stagnation and aging that accompany developed economies. Each row is a snapshot of millions of individual lives aggregated into a single number.

### Understanding population data

Population counts are harder to obtain than one might assume. Only a handful of countries conduct reliable censuses at regular intervals. Many estimates rely on birth and death registrations (which vary in completeness), surveys of representative samples (which involve statistical uncertainty), or projections from previous counts (which compound errors over time). The World Bank's task is to synthesize these imperfect sources into consistent estimates that allow comparison across countries and years.

```{r}
#| echo: false
countrypops |>
  dplyr::filter(year == 2023) |>
  dplyr::slice_max(population, n = 15) |>
  dplyr::select(country_name, country_code_3, population) |>
  gt(rowname_col = "country_name") |>
  fmt_number(columns = population, suffixing = TRUE) |>
  fmt_flag(columns = country_code_3) |>
  cols_label(
    country_code_3 = "",
    population = "Population (2023)"
  ) |>
  cols_width(country_code_3 ~ px(40)) |>
  tab_header(
    title = "World's Most Populous Countries",
    subtitle = "2023 estimated population"
  )
```

The uncertainties in population data matter for policy and planning. A country that believes it has 100 million people will allocate resources differently than one that believes it has 120 million. Census undercounts (common in remote areas, among marginalized populations, and in places where people distrust government) lead to underinvestment in precisely the communities that need services most. The `countrypops` figures represent best estimates, not ground truth, and users should remember this limitation.

That said, the trends in population data are generally reliable even when the absolute numbers carry uncertainty. If the World Bank estimates that Nigeria's population doubled between 1990 and 2020, the actual growth was almost certainly substantial even if the precise figures might be revised. Trends matter more than point estimates for most analytical purposes, and the `countrypops` dataset captures these trends across the entire modern era of demographic record-keeping.

```{r}
#| echo: false
countrypops |>
  dplyr::filter(country_code_3 %in% c("DEU", "JPN", "ITA", "ESP", "KOR")) |>
  dplyr::filter(year >= 1990) |>
  dplyr::group_by(country_name) |>
  dplyr::mutate(
    pop_1990 = population[year == 1990],
    index = population / pop_1990 * 100
  ) |>
  dplyr::filter(year %in% c(1990, 2000, 2010, 2020)) |>
  dplyr::ungroup() |>
  dplyr::select(country_name, year, index) |>
  tidyr::pivot_wider(names_from = year, values_from = index) |>
  gt(rowname_col = "country_name") |>
  fmt_number(decimals = 1) |>
  tab_header(
    title = "Population Change in Aging Societies",
    subtitle = "Index: 1990 = 100"
  ) |>
  tab_source_note("Values show population relative to 1990 baseline")
```

The table above shows population indexed to 1990 for five countries facing demographic aging. Japan's population has declined in absolute terms. Germany and Italy have barely grown. South Korea's growth is slowing rapidly. These patterns reflect low birth rates, increased longevity, and (in some cases) restrictive immigration policies. The economic and social implications of aging populations (pension systems, healthcare costs, labor force composition) represent some of the most significant policy challenges of the coming decades.

The dataset updates whenever the World Bank publishes new estimates, rather than on any fixed release schedule. This ongoing maintenance means that examples in documentation and books remain current. A population figure for China in 2024 becomes available, and shortly thereafter it appears in `countrypops`. This currency makes the dataset more useful for teaching than static historical data would be.

For **gt** demonstrations, `countrypops` excels at time series comparisons, geographic groupings, and the handling of large numbers. The population values range from thousands (small island nations) to billions (China and India), exercising formatters across their full dynamic range. The longitudinal structure supports year-over-year comparisons, growth rate calculations, and the kind of decade-by-decade summary tables that appear in demographic reports.

## `towny`

While `countrypops` takes a global view, `towny` focuses on a single Canadian province: Ontario. The dataset contains population figures for 414 municipalities, including data from every Canadian census between 1996 and 2021 (conducted every five years) plus various geographic and administrative attributes. It exists because I actually live in Ontario and wanted an excuse to know more about the places surrounding me.

```{r}
towny |>
  dplyr::select(name, land_area_km2, population_2001, population_2021, density_2021) |>
  dplyr::slice_max(population_2021, n = 10) |>
  gt(rowname_col = "name") |>
  fmt_integer(columns = c(population_2001, population_2021)) |>
  fmt_number(columns = land_area_km2, decimals = 1) |>
  fmt_number(columns = density_2021, decimals = 1) |>
  cols_label(
    land_area_km2 = "Area (km²)",
    population_2001 = "Pop. 2001",
    population_2021 = "Pop. 2021",
    density_2021 = "Density"
  )
```

The data comes from Statistics Canada and reveals patterns that might surprise those unfamiliar with Canadian geography. Toronto dominates, of course, but the surrounding municipalities (Mississauga, Brampton, Hamilton) have grown substantially over twenty years. Some smaller towns have declined as economic opportunities concentrated elsewhere. The dataset captures this quiet drama of population redistribution that plays out across every country's regions.

Ontario municipality names offer their own entertainment. Some are indigenous place names with beautiful sounds. Others commemorate British royalty or colonial administrators. A few seem almost whimsical when encountered for the first time. These names appear on highway signs and maps, marking places where real communities exist with their own histories and concerns. The `towny` dataset transforms those signs into data, inviting exploration of what lies behind the familiar names.

```{r}
#| echo: false
towny |>
  dplyr::mutate(
    growth_rate = (population_2021 - population_2001) / population_2001 * 100
  ) |>
  dplyr::filter(population_2001 > 10000) |>
  dplyr::slice_max(growth_rate, n = 10) |>
  dplyr::select(name, population_2001, population_2021, growth_rate) |>
  gt(rowname_col = "name") |>
  fmt_integer(columns = c(population_2001, population_2021)) |>
  fmt_number(columns = growth_rate, decimals = 1, pattern = "{x}%") |>
  cols_label(
    population_2001 = "Pop. 2001",
    population_2021 = "Pop. 2021",
    growth_rate = "Growth"
  ) |>
  tab_header(
    title = "Fastest Growing Ontario Municipalities",
    subtitle = "Among places with 10,000+ residents in 2001"
  )
```

The fastest-growing municipalities cluster around the Greater Toronto Area, where housing demand has driven expansion into formerly rural townships. Milton, Brampton, and Markham have transformed from small towns into substantial cities within a generation. The infrastructure challenges of this growth (roads, schools, healthcare, transit) consume enormous resources and dominate local politics. The `towny` data captures the before and after of this transformation but cannot convey the lived experience of watching farmland become subdivisions.

Not every municipality grew. Some communities in northern and eastern Ontario lost population as young people left for opportunities elsewhere. Factory closures, mine exhaustions, and the general drift of economic activity toward metropolitan areas hollowed out places that had thrived in earlier decades. The dataset does not distinguish between population loss from out-migration and loss from natural decrease (more deaths than births), but both dynamics contribute to the patterns visible in the numbers.

For table-making, `towny` provides opportunities for population density calculations, before-after comparisons, and growth rate analysis across its six census years. The land area column enables density visualization. The municipality names work naturally as row labels in grouped tables organized by population tier or geographic region.

## `peeps`

The `peeps` dataset contains fictional personal information for 100 imaginary people: names, addresses, phone numbers, email addresses, and nationalities. These fake individuals were generated using an online tool that produces realistic-seeming demographic data, then verified for plausible formatting of addresses and contact information across different countries.


The `gibraltar` dataset contains hourly weather observations from Gibraltar during May 2023: temperature, humidity, wind speed, cloud cover, and other meteorological variables. It provides 744 rows representing each hour of a single month in this small but fascinating territory.

```{r}
gibraltar |>
  dplyr::slice_sample(n = 8) |>
  dplyr::select(date, time, temp, humidity, wind_speed, wind_dir) |>
  gt() |>
  fmt_number(columns = temp, decimals = 1) |>
  fmt_integer(columns = c(humidity, wind_speed)) |>
  cols_label(
    date = "Date",
    time = "Time", 
    temp = "Temp (°C)",
    humidity = "Humidity %",
    wind_speed = "Wind (km/h)",
    wind_dir = "Direction"
  )
```

Gibraltar sits at the southern tip of the Iberian Peninsula, a British Overseas Territory of barely seven square kilometers guarding the entrance to the Mediterranean Sea. It is the kind of place that captures the imagination precisely because it seems improbable: a limestone promontory with its own airport runway crossing the main road, Barbary macaques roaming the upper rock, and a rather complex history.


## `rx_adsl` and `rx_addv`

These two datasets represent **gt**'s connection to the pharmaceutical industry, where clinical trial tables must meet rigorous standards for regulatory submission. The datasets follow CDISC (Clinical Data Interchange Standards Consortium) conventions, specifically the ADaM (Analysis Data Model) structure used throughout the pharmaceutical industry.

`rx_adsl` contains subject-level data (ADSL format) for 182 participants in a fictional clinical trial. `rx_addv` provides protocol deviation records (ADDV format) with 910 entries documenting when and how trial participants deviated from study protocols. Both datasets use standard variable names and coding conventions that pharmaceutical statisticians will immediately recognize.

```{r}
rx_adsl |>
  dplyr::select(USUBJID, AGE, SEX, ETHNIC, TRTA) |>
  dplyr::slice_head(n = 8) |>
  gt() |>
  cols_label(
    USUBJID = "Subject ID",
    AGE = "Age",
    SEX = "Sex",
    ETHNIC = "Ethnicity",
    TRTA = "Treatment"
  )
```

### The language of clinical trials

Pharmaceutical data follows conventions that seem arcane to outsiders but enable precise communication among specialists. USUBJID uniquely identifies a subject across all studies from a sponsor. TRTA indicates the actual treatment received (as opposed to the treatment assigned). SAFFL flags subjects in the safety population. This vocabulary, defined by CDISC standards, appears in regulatory submissions worldwide. A statistician in Switzerland reviewing a submission from Japan knows exactly what TRTA means because the standards are universal.

```{r}
#| echo: false
rx_adsl |>
  dplyr::group_by(TRTA) |>
  dplyr::summarize(
    subjects = n(),
    avg_age = mean(AGE, na.rm = TRUE),
    pct_female = mean(SEX == "F", na.rm = TRUE) * 100,
    ethnicities = n_distinct(ETHNIC),
    .groups = "drop"
  ) |>
  gt(rowname_col = "TRTA") |>
  fmt_number(columns = avg_age, decimals = 1) |>
  fmt_number(columns = pct_female, decimals = 0, pattern = "{x}%") |>
  cols_label(
    subjects = "Subjects",
    avg_age = "Mean Age",
    pct_female = "Female",
    ethnicities = "Ethnicities"
  ) |>
  tab_header(title = "Treatment Arm Demographics")
```

The treatment arms in clinical trials typically include the experimental treatment at one or more doses, a placebo or active comparator, and sometimes multiple dosing regimens. Demographic balance across arms helps ensure that observed differences reflect treatment effects rather than baseline differences. Age, sex, ethnicity, disease severity at baseline, and prior treatments all require documentation and comparison.

```{r}
#| echo: false
rx_addv |>
  dplyr::count(DVCAT) |>
  dplyr::arrange(desc(n)) |>
  head(8) |>
  gt(rowname_col = "DVCAT") |>
  cols_label(n = "Deviations") |>
  tab_header(title = "Protocol Deviation Categories")
```

Protocol deviations document when trial participants did not follow the study plan. Some deviations are minor (a visit occurring outside the allowed window). Others are major (taking prohibited medications, missing doses). The `rx_addv` dataset catalogs these deviations, enabling sensitivity analyses that exclude subjects with major violations. Regulators scrutinize deviation patterns for evidence that the trial was conducted properly and that deviations do not undermine the conclusions.

These datasets were contributed by Alexandra Lauer as part of ongoing collaboration between **gt** developers and pharmaceutical industry users. The package website includes a dedicated case study article demonstrating how to create clinical tables that meet industry standards. For pharmaceutical statisticians evaluating **gt** for regulatory work, these datasets provide immediately relevant examples.

The inclusion of pharmaceutical data reflects **gt**'s ambition to serve professional communities with specialized requirements. Clinical trials generate enormous quantities of tabular output, much of it following strict formatting conventions. Having sample datasets in the standard format lowers the barrier for pharmaceutical users to adopt **gt** and verify that it meets their needs.

## The value of curated datasets

Looking across all eighteen datasets, certain patterns emerge. Many fill gaps where public data existed but not in convenient form. The scientific datasets consolidate information scattered across journals and government pages. The `films` dataset creates a resource that simply did not exist before. Even the simulated `pizzaplace` data serves a purpose: realistic transactional data is rarely available publicly due to business confidentiality.

Other datasets reflect personal curiosity. Ontario towns. The Paris Métro. Gibraltar's weather. Cannes films. These choices say something about my interests and the particular corners of the world that captured my attention. The datasets are better for this personal investment. Someone who cares about the Paris Métro will notice details that a disinterested compiler would miss.

For users of **gt**, the datasets provide reliable materials for learning and experimentation. The variety ensures that nearly any table type (financial, scientific, demographic, geographic, categorical) has relevant sample data available. The careful construction means edge cases are present: missing values, unusual formatting requirements, multi-valued fields. The documentation grounds each dataset in context that makes the data more meaningful to work with.

Datasets are infrastructure. Good ones get used for years, appearing in examples, tutorials, homework assignments, and documentation. The eighteen datasets in **gt** aspire to that longevity. They are not throwaway data generated to fill a requirement but carefully assembled resources intended to remain useful across many versions and use cases. The stories behind them, now recorded here, add another layer of value: not just what the data contains but why it exists and where it came from.

