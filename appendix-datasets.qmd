# The gt datasets {#sec-appendix-datasets}

```{r setup, include=FALSE, echo=FALSE}
library(gt)
library(dplyr)
```

Every dataset tells a story. The eighteen datasets bundled with **gt** were not assembled arbitrarily or pulled from dusty archives of convenient CSV files. Each one emerged from a specific need, a personal curiosity, or a gap in what was publicly available. Some model human behavior with surprising fidelity. Others preserve scientific data that would otherwise remain scattered across obscure government pages. A few exist simply because I thought "I wish this dataset existed" and then made it so.

This appendix takes you behind the scenes of these datasets. You will learn where they came from, how they were constructed, and why they matter beyond their immediate utility as demonstration data. Along the way, we will explore the broader contexts they represent: the economics of a neighborhood pizza shop, the 125-year evolution of urban transit in Paris, the delicate chemistry of Earth's atmosphere, and the quiet dramas of population change in Ontario's small towns. The datasets are more than rows and columns. They are windows into worlds worth understanding.

## `pizzaplace`

The `pizzaplace` dataset contains 49,574 rows representing every pizza sold at a fictional pizzeria during the year 2015. Each row records a transaction with its timestamp, pizza type, size, and price. On the surface it appears to be straightforward sales data. Underneath, it is an elaborate simulation of human behavior, kitchen operations, and the unpredictable rhythms of a small food business.

The inspiration for this dataset came from Plateau Pizza, a real establishment in Coquitlam, British Columbia. The restaurant occupies a pleasant spot in a suburban plaza alongside a dollar store called Dollars & Cents and an IGA grocery store. It is the kind of neighborhood pizza place that survives on regulars and convenience, offering pizzas with names that are equal parts cheesy and memorable. Goat Supreme. The Calabrese. Names that stick in your head even if you cannot quite remember what toppings they included.

The dataset borrowed liberally from Plateau Pizza's menu. Their category structure (Classic, Supreme, Veggie and Vegan, Chicken) was adopted directly. Many pizza names and ingredient combinations came straight from their website, though some were embellished and others invented entirely. The fictional additions drew inspiration from Food Network recipes and local salumist offerings that seemed appropriately gourmet. Pricing followed a similar pattern: real prices served as the baseline, then adjustments were made based on perceived ingredient costs. The fancier cheeses and cured meats commanded premium prices, as they should.

What makes `pizzaplace` genuinely interesting is not the menu but the simulation that generated a year's worth of orders. The modeling script (preserved in the gt package's source repository at `data-raw/05-pizzaplace.R`) creates synthetic customers who arrive throughout each day with realistic patterns. Barret Schloerke contributed refinements to the behavioral model, adding nuance to timing and preference distributions. Weekends see more traffic than weekdays, reflecting the work-leisure split that governs so much of consumer behavior. Holidays disrupt normal patterns in expected ways.

The simulation also includes what might be called narrative events. A kitchen fire at one point disrupts operations. These partially catastrophic incidents add verisimilitude to what would otherwise be suspiciously smooth data. Real businesses experience interruptions, equipment failures, staff shortages, and the occasional minor disaster. The `pizzaplace` data reflects this messiness, making it more valuable for realistic analysis exercises than perfectly clean synthetic data would be.

The dataset originally included plans for appetizers, side dishes, and beverages, but these were ultimately cut in favor of simplicity. A pizzeria that only sells pizza is easier to understand and analyze than one with a full menu. The constraint also keeps the focus on what makes the dataset distinctive: the careful modeling of how people order pizza throughout a year.

### The pizzas of pizzaplace

```{r}
#| echo: false

pizza_names <- c(
  classic_dlx = "The Classic Deluxe Pizza",
  big_meat = "The Big Meat Pizza",
  pepperoni = "The Pepperoni Pizza",
  hawaiian = "The Hawaiian Pizza",
  pep_msh_pep = "The Pepperoni, Mushroom, and Peppers Pizza",
  ital_cpcllo = "The Italian Capocollo Pizza",
  napolitana = "The Napolitana Pizza",
  the_greek = "The Greek Pizza",
  thai_ckn = "The Thai Chicken Pizza",
  bbq_ckn = "The Barbecue Chicken Pizza",
  southw_ckn = "The Southwest Chicken Pizza",
  cali_ckn = "The California Chicken Pizza",
  ckn_pesto = "The Chicken Pesto Pizza",
  ckn_alfredo = "The Chicken Alfredo Pizza",
  brie_carre = "The Brie Carre Pizza",
  calabrese = "The Calabrese Pizza",
  soppressata = "The Soppressata Pizza",
  sicilian = "The Sicilian Pizza",
  ital_supr = "The Italian Supreme Pizza",
  peppr_salami = "The Pepper Salami Pizza",
  prsc_argla = "The Prosciutto and Arugula Pizza",
  spinach_supr = "The Spinach Supreme Pizza",
  spicy_ital = "The Spicy Italian Pizza",
  mexicana = "The Mexicana Pizza",
  four_cheese = "The Four Cheese Pizza",
  five_cheese = "The Five Cheese Pizza",
  spin_pesto = "The Spinach Pesto Pizza",
  veggie_veg = "The Vegetables + Vegetables Pizza",
  green_garden = "The Green Garden Pizza",
  mediterraneo = "The Mediterranean Pizza",
  spinach_fet = "The Spinach and Feta Pizza",
  ital_veggie = "The Italian Vegetables Pizza"
)

pizzaplace |>
  dplyr::distinct(type, name, size, price) |>
  dplyr::mutate(size = factor(size, levels = c("S", "M", "L", "XL", "XXL"))) |>
  dplyr::arrange(type, name, size) |>
  dplyr::group_by(type, name) |>
  dplyr::summarize(
    sizes = paste(size, collapse = ", "),
    price_range = paste0(
      "$", sprintf("%.2f", min(price)), 
      " - $", sprintf("%.2f", max(price))
    ),
    .groups = "drop"
  ) |>
  dplyr::mutate(
    pizza_label = paste0(
      pizza_names[name],
      "<br><span style='font-family: monospace; font-size: 0.85em; color: gray;'>",
      name,
      "</span>"
    )
  ) |>
  dplyr::select(type, pizza_label, sizes, price_range) |>
  dplyr::arrange(type, pizza_label) |>
  gt(groupname_col = "type") |>
  fmt_markdown(columns = pizza_label) |>
  cols_label(
    pizza_label = "Pizza",
    sizes = "Available Sizes",
    price_range = "Price Range"
  ) |>
  tab_header(
    title = "The Complete pizzaplace Menu",
    subtitle = "All 32 pizza varieties by category"
  )
```

The menu reveals the character of the fictional establishment. Classic pizzas stick to familiar territory: pepperoni, Hawaiian, the combinations everyone recognizes. The Supreme category ventures into more elaborate ingredient lists with names that promise indulgence. Veggie options cater to those avoiding meat, while the Chicken category builds meals around that particular protein. Each pizza comes in multiple sizes, though not every pizza is available in every size. The XL (and XXL!) option appears only for a certain pizza, and presumably that one pizza is popular enough to warrant the larger formats (could be that the ingredient mix works well at scale).

Pricing follows intuitive logic. A basic cheese pizza costs less than one loaded with prosciutto and artichoke hearts. Size increases bring proportional price increases, though the per-square-inch cost typically decreases as you go larger (the eternal economic argument for ordering the bigger pizza). These pricing patterns create natural opportunities for analysis: which pizzas generate the most revenue? Which sizes sell best and for which types? How does day of week affect the popularity of the vegetarian options?

### A pizza tier list

Any discussion of pizza invites opinions about which pizzas are best. The following tier list represents one possible ranking of the `pizzaplace` menu, from essential classics to more adventurous options that may not suit every palate. Reasonable people will disagree, and that disagreement is part of what makes pizza culture endlessly entertaining.

```{r}
#| echo: false

pizza_ingredients <- dplyr::tribble(
  ~name, ~ingredients, ~tier, ~notes,
  "The Pepperoni", "Mozzarella, Pepperoni, Tomato Sauce", "S", "The benchmark against which all pizzas are measured",
  "The Big Meat", "Bacon, Pepperoni, Italian Sausage, Chorizo, Mozzarella, Tomato Sauce", "S", "Maximalist approach that somehow works",
  "The Classic Deluxe", "Pepperoni, Mushrooms, Red Onions, Red Peppers, Bacon, Mozzarella, Tomato Sauce", "S", "Everything a pizza should be",
  "The Barbecue Chicken", "Barbecued Chicken, Red Peppers, Green Peppers, Tomatoes, Red Onions, Mozzarella, Barbecue Sauce", "A", "Sweet and savory in perfect balance",
  "The Hawaiian", "Sliced Ham, Pineapple, Mozzarella, Tomato Sauce", "A", "Controversial but undeniably popular",
  "The Italian Capocollo", "Capocollo, Red Peppers, Tomatoes, Goat Cheese, Garlic, Oregano, Mozzarella, Tomato Sauce", "A", "Elevated ingredients lift the whole experience",
  "The Calabrese", "Nduja, Italian Sausage, Pepperoni, Tomatoes, Red Onions, Mozzarella, Tomato Sauce", "A", "Spicy nduja provides serious depth",
  "The Prosciutto", "Prosciutto, Arugula, Mozzarella, Tomato Sauce", "A", "Simple elegance, fresh and light",
  "The Four Cheese", "Ricotta, Gorgonzola, Romano, Mozzarella, Tomato Sauce", "B", "For dedicated cheese enthusiasts only",
  "The Vegetables", "Mushrooms, Tomatoes, Red Peppers, Green Peppers, Red Onions, Zucchini, Spinach, Garlic, Mozzarella, Tomato Sauce", "B", "The vegetable abundance can overwhelm",
  "The Spinach Pesto", "Spinach, Artichokes, Tomatoes, Sun-dried Tomatoes, Garlic, Pesto Sauce, Mozzarella", "B", "Pesto base divides opinion",
  "The Greek", "Kalamata Olives, Feta, Tomatoes, Red Onions, Red Peppers, Garlic, Mozzarella, Tomato Sauce", "B", "Mediterranean flavors work better as salad",
  "The Brie Carre", "Brie, Prosciutto, Caramelized Onions, Pears, Thyme, Garlic, Mozzarella", "C", "Ambitious but confused identity",
  "The Chicken Pesto", "Chicken, Tomatoes, Red Peppers, Spinach, Garlic, Pesto Sauce, Mozzarella", "C", "Pesto and chicken compete rather than complement",
  "The Soppressata", "Soppressata, Fontina, Mozzarella, Garlic, Tomato Sauce", "C", "Underwhelming given premium ingredients"
)

pizza_ingredients |>
  gt(groupname_col = "tier") |>
  cols_label(
    name = "Pizza",
    ingredients = "Ingredients",
    notes = "Assessment"
  ) |>
  tab_row_group(label = md("**S Tier** — Essential"), rows = tier == "S") |>
  tab_row_group(label = md("**A Tier** — Excellent"), rows = tier == "A") |>
  tab_row_group(label = md("**B Tier** — Good"), rows = tier == "B") |>
  tab_row_group(label = md("**C Tier** — Passable"), rows = tier == "C") |>
  row_group_order(groups = c(
    md("**S Tier** — Essential"),
    md("**A Tier** — Excellent"),
    md("**B Tier** — Good"),
    md("**C Tier** — Passable")
  )) |>
  tab_header(
    title = "The Definitive pizzaplace Tier List",
    subtitle = "A highly subjective ranking of 15 notable pizzas"
  ) |>
  tab_source_note("Rankings reflect one person's taste. YMMV.")
```

The S-tier pizzas represent the core of what a pizzeria should do well. The Pepperoni is the foundation, the pizza against which all others are implicitly compared. When someone says "let's get pizza", this is what most people imagine. The Big Meat takes maximalism seriously but avoids the trap of incoherence: bacon, pepperoni, sausage, and chorizo sound excessive, but they harmonize around a common meatiness (it really works!). The Classic Deluxe achieves balance, incorporating vegetables (mushrooms, peppers, onions) alongside meat without letting any single ingredient dominate.

The A-tier pizzas represent successful experiments. The Hawaiian remains perpetually controversial, inspiring passionate defenses and equally passionate condemnations. But I'm a fan. The combination of salty ham and sweet pineapple against tangy tomato sauce works for a lot of people, and the dataset shows it selling consistently throughout the year. The Italian Capocollo and Calabrese elevate the genre through premium cured meats, offering pizzas that could credibly appear on a trattoria menu. The Prosciutto demonstrates that restraint can be a virtue: just ham, arugula, and cheese, but executed with quality ingredients.

The B-tier pizzas are solid but not essential. The Four Cheese appeals to dedicated turophiles but can feel monotonous without the textural variety that vegetables or meats provide. The vegetable-heavy options (Vegetables, Greek, Spinach Pesto) often release too much moisture during cooking, resulting in soggy centers that undermine the crust. Don't get me wrong, they are fine pizzas but rarely anyone's first choice.

The C-tier pizzas represent experiments that did not quite succeed. The Brie Carre attempts a French-inspired combination of brie, pears, and prosciutto that sounds sophisticated but tastes confused. The ingredients come from different culinary traditions and never fully integrate. These pizzas *do* seem to sell in this particular dataset, but they probably rarely inspire repeat orders or enthusiastic recommendations.

### What makes a good pizza

The tier list above reflects accumulated pizza wisdom, but the principles underlying it deserve articulation. A good pizza balances several competing demands. The crust must be sturdy enough to support toppings without becoming soggy, yet thin and pliable enough to fold. The sauce should be present but not overwhelming, providing acidity and moisture without drowning the other flavors. The cheese needs to melt smoothly and brown slightly without becoming rubbery or releasing pools of grease. The toppings must be distributed evenly and cut to appropriate sizes so that each bite contains a representative sample. And freshness is non-negotiable. A pizza that has been languishing in a display case for hours, slowly drying out under a heat lamp, is a shadow of its former self. The crust toughens, the cheese congeals, and the toppings develop that dispiriting sheen of oxidation. At the extreme end of neglect, [one reviewer actually found mold growing on the underside of a display pizza](https://www.youtube.com/shorts/opoizv5lH0c), which is the sort of discovery that makes you reconsider every grab-and-go slice you have ever eaten.

Beyond these structural requirements, good pizza demonstrates restraint. The impulse to add more toppings is understandable but usually misguided. Each additional ingredient dilutes the impact of everything else. The best pizzas feature three to five components (beyond sauce and cheese) that complement one another. Pepperoni alone is better than pepperoni with six other meats. Mushrooms and peppers work together because their textures contrast and their flavors do not compete.

Temperature matters enormously. A pizza that has been sitting for twenty minutes bears little resemblance to one fresh from the oven. The cheese firms up, the crust softens, the toppings cool into sad disconnection. This is why pizzerias survive on dine-in and delivery rather than takeout that sits in a car. And do not even get me started on microwave pizza. The microwave does something deeply wrong to pizza crust, transforming it into a chewy, rubbery substance that no longer qualifies as bread by any reasonable definition. The cheese melts unevenly, the sauce superheats into lava pockets, and the whole experience leaves you wondering why you bothered. Cold leftover pizza eaten standing at the refrigerator at midnight is honestly preferable (I've actually grown to like it more and more these days). The pizzaplace dataset implicitly captures this reality: the simulation models customers who order and receive pizzas within a reasonable timeframe, not pizzas boxed and forgotten (and certainly not pizzas reheated in a microwave).

Finally, good pizza requires good ingredients. This seems obvious but explains why some inexpensive pizzas disappoint while others satisfy. The quality of the mozzarella matters. The tomatoes in the sauce matter. Even the olive oil brushed on the crust matters. Plateau Pizza, the real establishment that inspired the dataset, succeeds partly because it sources decent ingredients and does not cut corners that customers would notice. The fictional pizzaplace inherits this philosophy.

And yet, after all this talk of balance and restraint, it is worth acknowledging that pizza can abandon every one of these conventions and still be legitimate. A pizza can have no sauce. It can have no cheese. It can be nothing more than dough, olive oil, and anchovies. This sounds shocking to anyone raised on North American delivery pizza, but it is a perfectly valid expression of pizzadom with deep roots in Italian tradition. Pizza bianca, pizza marinara, and the various focaccia-adjacent flatbreads of southern Italy predate the mozzarella-laden versions by centuries. The rules above describe one tradition. Pizza is generous enough to accommodate others.

### Why pizza data matters

Pizza occupies a unique position in the landscape of consumer goods. It is simultaneously simple (bread, sauce, cheese, toppings) and infinitely variable. A pizzeria's menu encodes assumptions about its customers: their adventurousness, their price sensitivity, their dietary restrictions. Sales patterns encode behavior: when people eat, what they celebrate, how weather affects appetite.

Part of my motivation for creating this dataset was to draw attention to pizza analytics as a legitimate field of inquiry. The phrase sounds ridiculous, and that is partly the point. If a dataset can make someone smile while also teaching them time series decomposition and category performance analysis, it has done more work than a dataset that only accomplishes the second thing. Nobody has ever felt intimidated by pizza data. Nobody has ever opened a CSV of pizza orders and thought "I am not qualified to analyze this". The approachability is a feature (not a frivolity!).

The `pizzaplace` dataset serves as a sandbox for the kinds of analysis that businesses perform constantly. Revenue breakdowns, time series decomposition, category performance, seasonal adjustment. These techniques apply far beyond pizza. Anyone learning to analyze transactional data will find the patterns in `pizzaplace` transferable to retail, hospitality, and service industries generally. The dataset is large enough to be realistic (nearly 50,000 transactions) but small enough to process quickly on any modern computer.

For **gt** specifically, `pizzaplace` demonstrates grouped data, aggregation, currency formatting, and the presentation of time-based information. A year of pizza sales can become a monthly summary table, a daily heatmap, a ranked list of bestsellers, or a comparison across categories. The richness of the underlying data supports dozens of different table designs.

## `exibble`

The name `exibble` is a portmanteau of "example tibble" and it serves exactly that purpose. This tiny dataset of eight rows exists to demonstrate **gt**'s formatting capabilities without the distraction of meaningful content. Each column represents a different data type: numeric values, character strings, currency amounts, dates, times, datetimes, and logical values. Missing values appear in strategic locations to demonstrate `sub_missing()` and related substitution functions.

```{r}
gt(exibble)
```

The column names are deliberately generic (`num`, `char`, `currency`, `date`, `time`, `datetime`) because the content does not matter. What matters is having every common data type available in a single compact dataset. When documenting a date formatter, you need a date column. When showing number formatting options, you need numbers. When explaining how to handle NA values, you need NA values in predictable locations.

### Anatomy of a reference dataset

Each row and column in `exibble` was chosen to exercise different aspects of table formatting:

```{r}
#| echo: false
dplyr::tibble(
  Column = c("num", "char", "currency", "date", "time", "datetime", "row", "group"),
  Type = c("numeric", "character", "numeric", "Date", "character", "POSIXct", "character", "character"),
  Purpose = c(
    "Demonstrates numeric formatting across scales",
    "Provides recognizable text labels (fruits)",
    "Tests currency with decimals, zeros, NAs",
    "Shows date formatting patterns",
    "Character-encoded times for parsing",
    "Full datetime objects for formatting",
    "Stub/rowname labels for tables",
    "Group categories for row grouping"
  )
) |>
  gt(rowname_col = "Column") |>
  cols_label(
    Type = "R Type",
    Purpose = "Role in Examples"
  ) |>
  tab_header(title = "exibble Column Structure")
```

The fruit names in the `char` column (apricot, banana, coconut, and so forth) follow alphabetical order, which makes them easy to verify when demonstrating sorting or filtering operations. The numeric values span several orders of magnitude, from fractions to millions, ensuring that formatters must handle both small precise values and large rounded ones. The currency column includes a missing value and one very small amount, testing edge cases that might trip up naive formatting approaches.

The `row` and `group` columns transform `exibble` from a formatting showcase into a structural one. With `row` serving as a stub and `group` organizing rows into categories, the same eight-row dataset can demonstrate virtually every **gt** feature. Headers, stubs, row groups, column formatting, substitution, styling... all can be shown using just `exibble`.

```{r}
#| echo: false
exibble |>
  gt(rowname_col = "row", groupname_col = "group") |>
  fmt_number(columns = num, decimals = 2) |>
  fmt_currency(columns = currency, decimals = 2) |>
  fmt_date(columns = date, date_style = "yMd") |>
  fmt_time(columns = time, time_style = "Hm") |>
  fmt_datetime(columns = datetime, date_style = "yMd", time_style = "Hm") |>
  sub_missing(missing_text = "—") |>
  tab_header(
    title = "exibble with Row Groups and Stub",
    subtitle = "Demonstrating structural features"
  )
```

Datasets like `exibble` just don't get a lot of attention, however, they are essential as infrastructure for examples in documentation. Every example in **gt**'s documentation that needs to show a quick formatting demonstration reaches for `exibble` rather than constructing throwaway data inline. This consistency helps readers recognize the dataset and focus on what is being demonstrated rather than puzzling over unfamiliar data structures.

## `gtcars`

The `gtcars` dataset contains specifications for 47 luxury and performance automobiles, with an emphasis on grand touring vehicles. The name works on two levels: these are GT (grand tourer) cars, and the dataset lives in a package called **gt**. The wordplay is intentional but understated. I try not to make a big deal about it.

```{r}
gtcars |>
  dplyr::select(mfr, model, year, hp, trq, mpg_c, mpg_h, msrp) |>
  dplyr::slice_head(n = 10) |>
  gt(rowname_col = "model", groupname_col = "mfr") |>
  fmt_integer(columns = c(hp, trq)) |>
  fmt_currency(columns = msrp, decimals = 0) |>
  cols_label(
    year = "Year",
    hp = "HP",
    trq = "Torque",
    mpg_c = "MPG (city)",
    mpg_h = "MPG (hwy)",
    msrp = "MSRP"
  )
```

The dataset was assembled from Motor Trend articles about grand touring vehicles, with additional research filling in gaps for fuel economy and torque figures. Most vehicles date from around 2015, reflecting when the source articles were published. The selection criteria emphasized true grand tourers: vehicles designed for high-speed, long-distance driving in comfort, typically with powerful engines, refined interiors, and substantial price tags.

### What makes a grand tourer?

The grand touring concept originated in 1950s Europe, when wealthy motorists began taking extended driving holidays across the continent. A proper GT needed range (400+ kilometers between fuel stops), performance (for the autobahns and mountain passes), and comfort (for hours behind the wheel). The Ferrari 250 GT established the template: front-mounted V12, leather interior, elegant coachwork by Pininfarina or Scaglietti. The grand tourer was always as much about aspiration as transportation.

```{r}
#| echo: false
gtcars |>
  dplyr::group_by(mfr) |>
  dplyr::summarize(
    models = n(),
    avg_hp = mean(hp),
    avg_msrp = mean(msrp),
    avg_mpg = mean((mpg_c + mpg_h) / 2),
    .groups = "drop"
  ) |>
  dplyr::arrange(desc(avg_msrp)) |>
  head(10) |>
  gt(rowname_col = "mfr") |>
  fmt_integer(columns = c(models, avg_hp)) |>
  fmt_currency(columns = avg_msrp, decimals = 0) |>
  fmt_number(columns = avg_mpg, decimals = 1) |>
  cols_label(
    models = "Models",
    avg_hp = "Avg HP",
    avg_msrp = "Avg Price",
    avg_mpg = "Avg MPG"
  ) |>
  tab_header(
    title = "Manufacturers by Average Price",
    subtitle = "From the gtcars dataset"
  )
```

Ferrari commands the highest average prices in the dataset, followed by the other Italian marques and the boutique British manufacturers. The German manufacturers (BMW, Mercedes, Audi) cluster at somewhat lower price points but offer more models, representing a broader range from accessible sports cars to top-tier performance vehicles. American manufacturers appear in the dataset primarily through Ford (the GT supercar) and Chevrolet (the Corvette), carrying on a different tradition of performance cars with a more democratic price structure.

```{r}
#| echo: false
gtcars |>
  dplyr::filter(!is.na(hp) & !is.na(trq)) |>
  dplyr::mutate(power_tier = case_when(
    hp >= 600 ~ "Extreme (600+ HP)",
    hp >= 500 ~ "High (500-599 HP)",
    hp >= 400 ~ "Strong (400-499 HP)",
    TRUE ~ "Modest (<400 HP)"
  )) |>
  dplyr::group_by(power_tier) |>
  dplyr::summarize(
    count = n(),
    avg_torque = mean(trq),
    avg_price = mean(msrp),
    .groups = "drop"
  ) |>
  dplyr::mutate(
    power_tier = factor(
      power_tier,
      levels = c("Modest (<400 HP)", "Strong (400-499 HP)", "High (500-599 HP)", "Extreme (600+ HP)")
    )
  ) |>
  dplyr::arrange(power_tier) |>
  gt(rowname_col = "power_tier") |>
  fmt_integer(columns = c(count, avg_torque)) |>
  fmt_currency(columns = avg_price, decimals = 0) |>
  cols_label(
    count = "Models",
    avg_torque = "Avg Torque (lb-ft)",
    avg_price = "Avg Price"
  ) |>
  tab_header(title = "Performance Tiers")
```

The relationship between horsepower and price is neither linear nor deterministic. Some modest-horsepower vehicles (certain Porsches, for instance) command premium prices through brand cachet and driving dynamics. Some extremely powerful vehicles achieve their output through brute displacement rather than exotic engineering, keeping prices relatively accessible. The correlation exists, but the exceptions tell interesting stories.

The choice to create `gtcars` was motivated by a desire for a modern equivalent to the venerable `mtcars` dataset that has shipped with R for decades. The original `mtcars` contains 1974 Motor Trend data on 32 automobiles, and it has been used in countless examples and tutorials. But cars from 1974 feel increasingly remote from contemporary experience. A dataset of modern luxury vehicles offers familiar reference points (Ferrari, Porsche, Aston Martin) and specifications that relate to cars people actually see on roads today.

For table-making purposes, `gtcars` provides natural groupings by manufacturer, multiple numeric columns suitable for formatting and comparison, and a mix of discrete and continuous variables. The manufacturer and model columns enable row grouping and stub labeling. The price column practically demands currency formatting. The horsepower and torque columns work well for bar chart visualizations within cells. It is a dataset that seems designed for beautiful tables because, in fact, it was.

## `countrypops`

The `countrypops` dataset tracks population estimates for countries worldwide from 1960 through the present (currently extending to 2024). The data comes from the World Bank, which compiles demographic estimates from national statistical offices, census data, and the United Nations Population Division. With over 13,000 rows covering more than 200 countries across six decades, it is one of the larger datasets in the **gt** collection.

```{r}
countrypops |>
  dplyr::filter(country_code_3 %in% c("USA", "CHN", "IND", "BRA", "NGA")) |>
  dplyr::filter(year %in% c(1960, 1980, 2000, 2020)) |>
  dplyr::select(country_name, year, population) |>
  tidyr::pivot_wider(names_from = year, values_from = population) |>
  gt(rowname_col = "country_name") |>
  fmt_number(suffixing = TRUE) |>
  tab_header(title = "Population Growth in Five Major Nations")
```

Population data might seem straightforward, but it encodes profound stories of human migration, economic development, public health, and political change. China's population trajectory shows the demographic impact of the one-child policy. Nigeria's explosive growth reflects patterns common across sub-Saharan Africa. European countries exhibit the stagnation and aging that accompany developed economies. Each row is a snapshot of millions of individual lives aggregated into a single number.

### Understanding population data

Population counts are harder to obtain than one might assume. Only a handful of countries conduct reliable censuses at regular intervals. Many estimates rely on birth and death registrations (which vary in completeness), surveys of representative samples (which involve statistical uncertainty), or projections from previous counts (which compound errors over time). The World Bank's task is to synthesize these imperfect sources into consistent estimates that allow comparison across countries and years.

```{r}
#| echo: false
countrypops |>
  dplyr::filter(year == 2023) |>
  dplyr::slice_max(population, n = 15) |>
  dplyr::select(country_name, country_code_3, population) |>
  gt(rowname_col = "country_name") |>
  fmt_number(columns = population, suffixing = TRUE) |>
  fmt_flag(columns = country_code_3) |>
  cols_label(
    country_code_3 = "",
    population = "Population (2023)"
  ) |>
  cols_width(country_code_3 ~ px(40)) |>
  tab_header(
    title = "World's Most Populous Countries",
    subtitle = "2023 estimated population"
  )
```

The uncertainties in population data matter for policy and planning. A country that believes it has 100 million people will allocate resources differently than one that believes it has 120 million. Census undercounts (common in remote areas, among marginalized populations, and in places where people distrust government) lead to underinvestment in precisely the communities that need services most. The `countrypops` figures represent best estimates, not ground truth, and users should remember this limitation.

That said, the trends in population data are generally reliable even when the absolute numbers carry uncertainty. If the World Bank estimates that Nigeria's population doubled between 1990 and 2020, the actual growth was almost certainly substantial even if the precise figures might be revised. Trends matter more than point estimates for most analytical purposes, and the `countrypops` dataset captures these trends across the entire modern era of demographic record-keeping.

```{r}
#| echo: false
countrypops |>
  dplyr::filter(country_code_3 %in% c("DEU", "JPN", "ITA", "ESP", "KOR")) |>
  dplyr::filter(year >= 1990) |>
  dplyr::group_by(country_name) |>
  dplyr::mutate(
    pop_1990 = population[year == 1990],
    index = population / pop_1990 * 100
  ) |>
  dplyr::filter(year %in% c(1990, 2000, 2010, 2020)) |>
  dplyr::ungroup() |>
  dplyr::select(country_name, year, index) |>
  tidyr::pivot_wider(names_from = year, values_from = index) |>
  gt(rowname_col = "country_name") |>
  fmt_number(decimals = 1) |>
  tab_header(
    title = "Population Change in Aging Societies",
    subtitle = "Index: 1990 = 100"
  ) |>
  tab_source_note("Values show population relative to 1990 baseline")
```

The table above shows population indexed to 1990 for five countries facing demographic aging. Japan's population has declined in absolute terms. Germany and Italy have barely grown. South Korea's growth is slowing rapidly. These patterns reflect low birth rates, increased longevity, and (in some cases) restrictive immigration policies. The economic and social implications of aging populations (pension systems, healthcare costs, labor force composition) represent some of the most significant policy challenges of the coming decades.

The dataset updates whenever the World Bank publishes new estimates, rather than on any fixed release schedule. This ongoing maintenance means that examples in documentation and books remain current. A population figure for China in 2024 becomes available, and shortly thereafter it appears in `countrypops`. This currency makes the dataset more useful for teaching than static historical data would be.

For **gt** demonstrations, `countrypops` excels at time series comparisons, geographic groupings, and the handling of large numbers. The population values range from thousands (small island nations) to billions (China and India), exercising formatters across their full dynamic range. The longitudinal structure supports year-over-year comparisons, growth rate calculations, and the kind of decade-by-decade summary tables that appear in demographic reports.

## `towny`

While `countrypops` takes a global view, `towny` focuses on a single Canadian province: Ontario. The dataset contains population figures for 414 municipalities, including data from every Canadian census between 1996 and 2021 (conducted every five years) plus various geographic and administrative attributes. It exists because I actually live in Ontario and wanted an excuse to know more about the places surrounding me.

```{r}
towny |>
  dplyr::select(name, land_area_km2, population_2001, population_2021, density_2021) |>
  dplyr::slice_max(population_2021, n = 10) |>
  gt(rowname_col = "name") |>
  fmt_integer(columns = c(population_2001, population_2021)) |>
  fmt_number(columns = land_area_km2, decimals = 1) |>
  fmt_number(columns = density_2021, decimals = 1) |>
  cols_label(
    land_area_km2 = "Area (km²)",
    population_2001 = "Pop. 2001",
    population_2021 = "Pop. 2021",
    density_2021 = "Density"
  )
```

The data comes from Statistics Canada and reveals patterns that might surprise those unfamiliar with Canadian geography. Toronto dominates, of course, but the surrounding municipalities (Mississauga, Brampton, Hamilton) have grown substantially over twenty years. Some smaller towns have declined as economic opportunities concentrated elsewhere. The dataset captures this quiet drama of population redistribution that plays out across every country's regions.

Ontario municipality names offer their own entertainment. Some are indigenous place names with beautiful sounds. Others commemorate British royalty or colonial administrators. A few seem almost whimsical when encountered for the first time. These names appear on highway signs and maps, marking places where real communities exist with their own histories and concerns. The `towny` dataset transforms those signs into data, inviting exploration of what lies behind the familiar names.

```{r}
#| echo: false
towny |>
  dplyr::mutate(
    growth_rate = (population_2021 - population_2001) / population_2001 * 100
  ) |>
  dplyr::filter(population_2001 > 10000) |>
  dplyr::slice_max(growth_rate, n = 10) |>
  dplyr::select(name, population_2001, population_2021, growth_rate) |>
  gt(rowname_col = "name") |>
  fmt_integer(columns = c(population_2001, population_2021)) |>
  fmt_number(columns = growth_rate, decimals = 1, pattern = "{x}%") |>
  cols_label(
    population_2001 = "Pop. 2001",
    population_2021 = "Pop. 2021",
    growth_rate = "Growth"
  ) |>
  tab_header(
    title = "Fastest Growing Ontario Municipalities",
    subtitle = "Among places with 10,000+ residents in 2001"
  )
```

The fastest-growing municipalities cluster around the Greater Toronto Area, where housing demand has driven expansion into formerly rural townships. Milton, Brampton, and Markham have transformed from small towns into substantial cities within a generation. The infrastructure challenges of this growth (roads, schools, healthcare, transit) consume enormous resources and dominate local politics. The `towny` data captures the before and after of this transformation but cannot convey the lived experience of watching farmland become subdivisions.

Not every municipality grew. Some communities in northern and eastern Ontario lost population as young people left for opportunities elsewhere. Factory closures, mine exhaustions, and the general drift of economic activity toward metropolitan areas hollowed out places that had thrived in earlier decades. The dataset does not distinguish between population loss from out-migration and loss from natural decrease (more deaths than births), but both dynamics contribute to the patterns visible in the numbers.

For table-making, `towny` provides opportunities for population density calculations, before-after comparisons, and growth rate analysis across its six census years. The land area column enables density visualization. The municipality names work naturally as row labels in grouped tables organized by population tier or geographic region.

## `peeps`

The `peeps` dataset contains fictional personal information for 100 imaginary people: names, addresses, phone numbers, email addresses, and nationalities. These fake individuals were generated using an online tool that produces realistic-seeming demographic data, then verified for plausible formatting of addresses and contact information across different countries.

```{r}
peeps |>
  dplyr::select(name_given, name_family, email_addr, country) |>
  dplyr::slice_sample(n = 6) |>
  gt() |>
  cols_label(
    name_given = "First Name",
    name_family = "Last Name",
    email_addr = "Email",
    country = "Country"
  )
```

The international scope was intentional. `peeps` was created specifically to demonstrate formatters like `fmt_email()`, `fmt_country()`, and `fmt_flag()`. Having people from various countries ensures that flag icons and country name formatting can be shown in realistic contexts. An address book or contact directory table should contain international entries, and `peeps` provides exactly that.

### The problem of synthetic data

Generating realistic fake data is harder than it sounds. Names must fit cultural expectations (a person from Japan should have a Japanese name). Addresses must follow country-specific formats (postal codes before or after city names, province abbreviations versus full names). Phone numbers must have correct country codes and plausible internal structure. Email addresses must look like real email addresses while clearly being fictional.

```{r}
#| echo: false
peeps |>
  dplyr::count(country) |>
  dplyr::arrange(desc(n)) |>
  head(15) |>
  gt(rowname_col = "country") |>
  cols_label(n = "People") |>
  tab_header(title = "People by Country")
```

The country distribution in `peeps` emphasizes variety over statistical representativeness. Having multiple people from smaller countries ensures that formatting edge cases get tested. A dataset with 90 Americans and 10 others would not exercise international formatting as thoroughly as one with broader distribution.

```{r}
#| echo: false
peeps |>
  dplyr::mutate(email_domain = sub(".*@", "", email_addr)) |>
  dplyr::count(email_domain) |>
  dplyr::arrange(desc(n)) |>
  head(10) |>
  gt(rowname_col = "email_domain") |>
  cols_label(n = "Count") |>
  tab_header(title = "Email Domain Distribution")
```

The email domains follow patterns typical of real email usage: major providers dominate, with country-specific services appearing for non-English-speaking regions. This realism helps ensure that `fmt_email()` handles the variety of domain lengths and TLD formats that appear in actual contact databases.

Every person in the dataset is entirely fictional. The addresses do not correspond to real residences. The phone numbers should not connect to anyone. But the formatting follows authentic patterns for each country represented. A French address looks like a French address. A Japanese name follows Japanese naming conventions. This verisimilitude matters because formatting functions must handle real-world variation, and `peeps` provides test cases for that variation without compromising anyone's actual privacy.

## `sza` (solar zenith angles)

The `sza` dataset originates from atmospheric chemistry research, specifically from data tables published in textbooks by Pitts and Finlayson-Pitts. It records solar zenith angles (the angle between the sun and the vertical) across different latitudes and months. The original data came from a US government source that may no longer be online, but the values remain scientifically accurate and useful.

```{r}
sza |>
  dplyr::filter(latitude %in% c(0, 30, 60), !is.na(sza)) |>
  tidyr::pivot_wider(names_from = month, values_from = sza) |>
  gt(rowname_col = "tst") |>
  tab_row_group(label = "Equator (0°)", rows = latitude == 0) |>
  tab_row_group(label = "Mid-latitude (30°)", rows = latitude == 30) |>
  tab_row_group(label = "High latitude (60°)", rows = latitude == 60) |>
  fmt_number(decimals = 1) |>
  sub_missing(missing_text = "—") |>
  cols_hide(columns = latitude) |>
  tab_header(title = "Solar Zenith Angles by Latitude and Month")
```

Solar zenith angles matter because they determine how much solar radiation reaches Earth's surface and at what angle. This affects everything from climate modeling to photovoltaic panel efficiency to the rate of photochemical reactions in the atmosphere. At high latitudes in winter, the sun barely rises above the horizon (large zenith angles). At the equator, the sun passes nearly overhead at noon year-round (small zenith angles). The interplay between latitude, season, and time of day creates the patterns visible in the `sza` data.

### Why zenith angles matter

When the sun sits directly overhead, sunlight travels through the minimum possible amount of atmosphere before reaching the surface. As the sun moves toward the horizon, light must traverse increasingly long atmospheric paths. This path length, often expressed as "air mass" affects both the intensity and spectral composition of sunlight reaching the ground. Ultraviolet radiation attenuates more strongly through longer path lengths, which is why sunburns are most severe around solar noon in summer at lower latitudes.

```{r}
#| echo: false
sza |>
  dplyr::filter(!is.na(sza)) |>
  dplyr::filter(tst %in% c("0600", "0900", "1200", "1500", "1800")) |>
  dplyr::filter(latitude %in% c(0, 20, 40, 60)) |>
  dplyr::filter(month %in% c("jan", "apr", "jul", "oct")) |>
  tidyr::pivot_wider(names_from = month, values_from = sza) |>
  gt(rowname_col = "tst", groupname_col = "latitude") |>
  fmt_number(decimals = 0) |>
  sub_missing(missing_text = "—") |>
  tab_header(
    title = "Solar Zenith Angles Throughout the Day",
    subtitle = "By latitude and season"
  ) |>
  cols_label(
    jan = "Jan",
    apr = "Apr",
    jul = "Jul",
    oct = "Oct"
  )
```

The seasonal pattern emerges clearly at mid and high latitudes. January brings high zenith angles in the Northern Hemisphere as the sun traces its winter arc low in the southern sky. July reverses the pattern, with the sun climbing high overhead at noon. At the equator, seasonal variation is minimal: the sun is always nearly overhead at midday. These patterns have shaped human civilization, determining growing seasons, driving migration patterns, and inspiring astronomical observations throughout history.

Atmospheric chemists care about zenith angles because photochemical reactions require light. Photolysis rates vary with the intensity and spectrum of incoming solar radiation. A nitrogen dioxide molecule that might photolyze within seconds at tropical noon may persist for hours at polar twilight. Models simulating urban smog formation or stratospheric ozone depletion must account for these variations, typically by looking up appropriate photolysis rates from tables indexed by zenith angle and altitude.

The dataset reflects my background in atmospheric chemistry. Such data tables would be directly imported into atmospheric box models for simulating photolysis-based reactions of volatile organic compounds (VOCs). Having the data readily available in R format eliminates the tedious work of transcribing values from printed tables or scraping data from web pages.

For **gt**, the `sza` dataset demonstrates heatmap-style coloring (values naturally vary from low to high in meaningful patterns), missing value handling (the sun does not rise at certain latitude-time combinations), and the presentation of scientific lookup tables. The structure (rows indexed by time, columns by month, grouped by latitude) maps naturally onto the table designs that scientists create when presenting such reference data.

## `constants`, `reactions`, `photolysis`, and `nuclides`

These four datasets form a cluster of scientific reference data, each addressing a gap in publicly available resources. While the underlying information exists in various forms (government pages, textbooks, specialized databases), it had not been consolidated into convenient R data frames. The **gt** package provided an opportunity to change that.

The `constants` dataset contains 30 fundamental physical constants with their values, units, and uncertainties. These are the numbers that appear in physics and chemistry textbooks: the speed of light, Planck's constant, Avogadro's number, the gravitational constant. Each value comes with associated metadata specifying units and measurement precision.

```{r}
constants |>
  dplyr::select(name, value, uncert, units) |>
  dplyr::slice_head(n = 8) |>
  gt() |>
  fmt_scientific(columns = c(value, uncert)) |>
  cols_label(
    name = "Constant",
    value = "Value",
    uncert = "Uncertainty",
    units = "Units"
  )
```

### The certainty of physical constants

Physical constants occupy a peculiar position in science. They are measured quantities, subject to experimental uncertainty, yet they describe fundamental features of the universe that we believe to be genuinely constant. The speed of light in vacuum, for instance, is now defined as exactly 299,792,458 meters per second, but this definition only became possible after decades of increasingly precise measurements. Before 1983, we measured the speed of light; now the meter is defined in terms of light's speed. The historical progression of uncertainty shrinking toward zero tells a story of experimental ingenuity.

```{r}
#| echo: false
constants |>
  dplyr::filter(!is.na(value) & !is.na(uncert)) |>
  dplyr::mutate(
    rel_uncert = uncert / abs(value),
    precision_class = case_when(
      rel_uncert < 1e-10 ~ "Extraordinarily precise",
      rel_uncert < 1e-8 ~ "Extremely precise",
      rel_uncert < 1e-6 ~ "Very precise",
      TRUE ~ "Moderately precise"
    )
  ) |>
  dplyr::group_by(precision_class) |>
  dplyr::summarize(
    count = n(),
    example = first(name),
    .groups = "drop"
  ) |>
  gt(rowname_col = "precision_class") |>
  cols_label(
    count = "Constants",
    example = "Example"
  ) |>
  tab_header(title = "Physical Constants by Measurement Precision")
```

The measurement precision varies dramatically across constants. The fine-structure constant (approximately 1/137) has been measured to extraordinary precision through quantum electrodynamics experiments. The gravitational constant, despite being one of the first constants ever measured (by Cavendish in 1798), remains relatively imprecise because gravity is so weak that experiments must contend with tiny forces and correspondingly large relative uncertainties.

### Atmospheric chemistry

The `reactions` dataset catalogs 1,344 atmospheric chemical reactions with their rate constants and temperature dependencies. The `photolysis` dataset provides photolysis rates for organic compounds, including spectral data stored in list columns. These datasets rarely exist in such accessible form. Researchers typically extract reaction rates from individual journal articles or specialized databases with restrictive access. Having a curated collection in R format simplifies atmospheric modeling exercises.

```{r}
#| echo: false
reactions |>
  dplyr::filter(!is.na(OH_k298)) |>
  dplyr::slice_sample(n = 8) |>
  dplyr::select(cmpd_name, cmpd_formula, OH_k298) |>
  gt() |>
  fmt_scientific(columns = OH_k298) |>
  fmt_chem(columns = cmpd_formula) |>
  cols_label(
    cmpd_name = "Compound",
    cmpd_formula = "Formula",
    OH_k298 = "OH Rate at 298K"
  ) |>
  tab_header(title = "Selected Atmospheric Reactions with OH")
```

Understanding atmospheric chemistry requires knowing how fast different reactions proceed under various conditions. The hydroxyl radical (OH) drives much of daytime atmospheric chemistry, attacking volatile organic compounds and beginning their oxidation chains. Nitrate radicals take over at night. Photolysis reactions require sunlight, their rates varying with solar zenith angle and wavelength. Each rate constant in the dataset represents experimental determinations, often from smog chamber studies or theoretical calculations validated against field measurements.

### Nuclear data

The `nuclides` dataset compiles nuclear data for isotopes: half-lives, decay modes, particle emissions. Like the chemical datasets, this information exists scattered across various sources but had not been unified into a single convenient data frame. Nuclear chemistry courses and research often require looking up isotope properties, and `nuclides` consolidates those lookups.

```{r}
#| echo: false
nuclides |>
  dplyr::filter(!is.na(half_life) & half_life > 0) |>
  dplyr::filter(decay_1 %in% c("B-", "B+", "A", "EC")) |>
  dplyr::group_by(decay_1) |>
  dplyr::summarize(
    count = n(),
    .groups = "drop"
  ) |>
  dplyr::mutate(
    decay_name = case_when(
      decay_1 == "B-" ~ "Beta minus",
      decay_1 == "B+" ~ "Beta plus",
      decay_1 == "A" ~ "Alpha",
      decay_1 == "EC" ~ "Electron capture",
      TRUE ~ decay_1
    )
  ) |>
  dplyr::select(decay_name, count) |>
  gt(rowname_col = "decay_name") |>
  cols_label(count = "Nuclides") |>
  tab_header(title = "Radioactive Decay Modes")
```

Radioactive decay follows several pathways depending on the nuclear configuration. Beta-minus decay converts a neutron to a proton, moving the nucleus one step higher in atomic number. Alpha decay ejects a helium nucleus, reducing both atomic number and mass. Electron capture pulls an inner-shell electron into the nucleus. Each decay mode appears in the dataset, along with the half-life governing how quickly unstable nuclei transform.

For **gt** demonstrations, these scientific datasets showcase formatters like `fmt_scientific()`, `fmt_chem()`, and `fmt_units()`. They also demonstrate `from_column()` usage, where the number of decimal places might come from an adjacent precision column rather than being hard-coded. Scientific communication demands careful attention to significant figures and uncertainty, and these datasets provide realistic contexts for that attention.

## `sp500`

The `sp500` dataset contains daily stock market data for the S&P 500 index from 1950 through 2015: opening price, closing price, high, low, and trading volume for each trading day. With over 16,000 rows, it provides a substantial corpus of financial time series data.

```{r}
sp500 |>
  dplyr::filter(date >= "2015-01-01") |>
  dplyr::slice_head(n = 10) |>
  gt(rowname_col = "date") |>
  fmt_currency(columns = c(open, high, low, close), decimals = 2) |>
  fmt_number(columns = volume, suffixing = TRUE) |>
  cols_label(
    open = "Open",
    high = "High",
    low = "Low",
    close = "Close",
    volume = "Volume"
  )
```

Financial data demands specific formatting conventions: currency symbols, appropriate decimal precision, volume abbreviations. The `sp500` dataset exercises these requirements across decades of market history. Bull markets, bear markets, crashes, and recoveries all appear in the data. The 1987 Black Monday crash, the dot-com bubble, the 2008 financial crisis: each left its signature in closing prices and trading volumes.

### Reading the market's history

The S&P 500 tracks 500 large-cap American companies, weighted by market capitalization. It serves as the benchmark against which most US equity investments are measured. A fund that "beats the market" outperforms the S&P 500. An investor who wants "market returns" buys an index fund tracking the S&P 500. The index represents the collective judgment of millions of market participants about the value of corporate America.

```{r}
#| echo: false
sp500 |>
  dplyr::mutate(year = as.integer(format(date, "%Y"))) |>
  dplyr::group_by(year) |>
  dplyr::summarize(
    trading_days = n(),
    year_open = first(open),
    year_close = last(close),
    year_high = max(high),
    year_low = min(low),
    avg_volume = mean(volume),
    .groups = "drop"
  ) |>
  dplyr::filter(year >= 2006 & year <= 2015) |>
  dplyr::mutate(
    return_pct = (year_close - year_open) / year_open * 100
  ) |>
  dplyr::select(year, year_open, year_close, return_pct, year_high, year_low) |>
  gt(rowname_col = "year") |>
  fmt_currency(columns = c(year_open, year_close, year_high, year_low), decimals = 0) |>
  fmt_number(columns = return_pct, decimals = 1, force_sign = TRUE, pattern = "{x}%") |>
  cols_label(
    year_open = "Open",
    year_close = "Close",
    return_pct = "Return",
    year_high = "Year High",
    year_low = "Year Low"
  ) |>
  tab_header(
    title = "S&P 500 Annual Summary",
    subtitle = "2006-2015"
  ) |>
  data_color(
    columns = return_pct,
    palette = c("#D73027", "#FFFFFF", "#1A9850"),
    domain = c(-40, 40)
  )
```

The years 2006 through 2015 illustrate the market's capacity for dramatic swings. 2008 stands out with its catastrophic decline during the financial crisis, when the index lost more than a third of its value. The subsequent years show gradual recovery, with the index reaching new highs by the early 2010s. Anyone who sold during the panic of 2008 locked in losses. Anyone who held through the crisis recovered and then some. The data tells both stories depending on which slices you examine.

The dataset originated from a web search that turned up historical market data, possibly compiled from Kaggle or similar sources. The exact provenance matters less than the utility: a long time series of financial data in a format ready for analysis and visualization. For teaching purposes, the `sp500` dataset provides realistic data for demonstrating time series analysis, returns calculations, volatility measurement, and the kind of financial tables that appear in annual reports and investment presentations.

## `metro`

The Paris Métro is one of the world's great urban transit systems. Opened in 1900, it has grown to 16 lines serving 308 stations across the city and surrounding communes. The `metro` dataset captures this network: station names, locations, line assignments, opening dates, and ridership figures.

```{r}
metro |>
  dplyr::select(name, lines, passengers) |>
  dplyr::slice_max(passengers, n = 10) |>
  gt(rowname_col = "name") |>
  fmt_number(columns = passengers, suffixing = TRUE) |>
  cols_label(
    lines = "Lines",
    passengers = "Annual Passengers"
  ) |>
  tab_header(title = "Busiest Paris Métro Stations")
```

The dataset exists because I genuinely admire the Paris Métro. Among the world's subway systems, it stands out for its density, connectivity, and integration with other transit modes (RER commuter rail, buses, trams, and high-speed TGV connections). The wayfinding and signage are exemplary. The expansion plans are ambitious and consistently executed. It represents what urban transit can be when treated as essential infrastructure rather than an afterthought.

### A brief history of the Métro

The story of the Paris Métro begins in the late nineteenth century, when Paris faced the same urban transportation crisis that afflicted every growing industrial city. Horse-drawn omnibuses clogged the boulevards. The wealthy rode in private carriages while workers walked miles to reach their jobs. London had opened its Underground in 1863, demonstrating that subterranean railways could move masses of people efficiently. Paris, perennially competitive with its cross-Channel rival, needed its own solution.

The first line opened on July 19, 1900, timed to coincide with the Exposition Universelle that drew millions of visitors to Paris that summer. Line 1 ran from Porte de Vincennes to Porte Maillot, connecting the eastern and western edges of the city through its commercial heart. The stations featured distinctive Art Nouveau entrances designed by Hector Guimard, with their sinuous cast-iron curves and amber glass panels that remain iconic more than a century later. Not all survived (many were removed during mid-century modernization campaigns and later regretted), but those that remain are protected monuments.

The network expanded rapidly in its early decades. By 1910, Paris had six lines. By 1920, ten. This breakneck pace was not entirely the product of unified planning. The Compagnie du chemin de fer métropolitain de Paris (CMP) held the primary concession, but it faced competition from the Nord-Sud Company, which built what would become Lines 12 and 13. The two companies raced to serve lucrative routes, and their rivalry accelerated construction beyond what a single monopoly might have achieved. The Nord-Sud stations were arguably more elegant, with ceramic tile work and distinctive lettering that enthusiasts still admire. When the companies merged in 1930, Paris inherited a network that had been built fast precisely because multiple actors were competing to build it.

This frenetic early growth distinguished Paris from its European peers. London's Underground, though older, expanded more cautiously under a patchwork of private companies that often duplicated routes rather than extending coverage. The Berlin U-Bahn, which opened in 1902, grew steadily but faced the complication of serving multiple municipalities that would not unify until 1920. Paris benefited from centralized city planning within the relatively compact boundaries of the twenty arrondissements, allowing the CMP and Nord-Sud to build a coherent network even while competing. By 1930, Paris had more stations than London despite London's forty-year head start.

The guiding philosophy was density: stations placed close together (often just 500 meters apart) so that no Parisian would have to walk more than a few minutes to reach the Métro. This density distinguishes Paris from systems like Washington DC or the Bay Area's BART, where stations are spaced miles apart and require feeder buses or long walks. The Paris approach sacrifices speed between stations for convenience of access, a tradeoff that makes sense for a compact, dense city.

```{r}
#| echo: false
dplyr::tibble(
  decade = c("1900s", "1910s", "1920s", "1930s", "1940s", "1950s", "1960s", "1970s", "1980s", "1990s", "2000s", "2010s", "2020s"),
  stations_opened = c(65, 85, 60, 25, 5, 15, 12, 8, 18, 10, 8, 14, 10),
  notable_events = c(
    "Line 1 opens for World's Fair",
    "Rapid expansion across Paris",
    "Network reaches most arrondissements",
    "Great Depression slows construction",
    "World War II occupation",
    "Post-war reconstruction begins",
    "RER regional express network planned",
    "RER lines A and B open",
    "Line 14 planning begins",
    "Line 14 construction starts",
    "Line 14 opens (first automated line)",
    "Line extensions to suburbs",
    "Grand Paris Express under construction"
  )
) |>
  gt(rowname_col = "decade") |>
  cols_label(
    stations_opened = "Stations Opened",
    notable_events = "Notable Events"
  ) |>
  tab_header(
    title = "125 Years of Métro Expansion",
    subtitle = "How the network grew decade by decade"
  )
```

The interwar period saw continued expansion but also financial difficulties. The 1930s depression slowed construction, and the network that had seemed destined for endless growth began to stabilize. World War II brought occupation and disruption. The Métro continued to operate (the Germans found it useful for moving troops and supplies), but expansion halted and maintenance suffered. Several stations were closed and converted to other uses, some serving as air raid shelters.

Post-war reconstruction proceeded slowly. The immediate decades after 1945 focused on repairing damage and updating aging infrastructure rather than building new lines. The real transformation came in the 1960s and 1970s with the creation of the RER (Réseau Express Régional), a network of express lines that tunneled through central Paris but extended far into the suburbs. The RER was not technically part of the Métro but integrated seamlessly with it, allowing commuters to transfer between the dense inner-city network and the faster regional lines.

### The modern network

Today's Métro comprises 16 lines totaling over 220 kilometers of track. The numbering seems haphazard (there are lines 1 through 14, plus 3bis and 7bis), reflecting historical accidents rather than logical planning. Lines 3bis and 7bis were originally branches of their parent lines that later gained operational independence. The system carries approximately 4 million passengers daily, making it one of the world's busiest rapid transit networks.

```{r}
#| echo: false
dplyr::tibble(
  line = c("1", "2", "3", "3bis", "4", "5", "6", "7", "7bis", "8", "9", "10", "11", "12", "13", "14"),
  color = c("#FFCD00", "#003CA6", "#837902", "#6EC4E8", "#CF009E", "#FF7E2E", "#6ECA97", "#FA9ABA", "#6ECA97", "#E19BDF", "#B6BD00", "#C9910D", "#704B1C", "#007852", "#6EC4E8", "#62259D"),
  length_km = c(16.6, 12.4, 11.7, 1.3, 12.1, 14.6, 13.6, 22.4, 3.1, 23.4, 19.6, 11.7, 6.3, 13.9, 24.3, 14.0),
  stations = c(25, 25, 25, 4, 27, 22, 28, 38, 8, 38, 37, 23, 13, 29, 32, 13),
  automated = c("Yes", "No", "No", "No", "Yes", "No", "No", "No", "No", "No", "No", "No", "No", "No", "No", "Yes")
) |>
  gt() |>
  cols_label(
    line = "Line",
    color = "",
    length_km = "Length (km)",
    stations = "Stations",
    automated = "Automated"
  ) |>
  fmt_number(columns = length_km, decimals = 1) |>
  data_color(
    columns = color,
    fn = function(x) x
  ) |>
  fmt(columns = color, fns = function(x) "") |>
  cols_width(color ~ px(30)) |>
  tab_header(
    title = "Paris Métro Lines",
    subtitle = "Current network statistics"
  )
```

Line 14 deserves special attention as the system's showcase. Opened in 1998, it was the first fully automated line on the network, operating without drivers. Platform screen doors prevent accidents (a significant concern on older lines) and allow trains to run with shorter headways. The stations feel modern and spacious compared to the cramped nineteenth-century tunnels of earlier lines. Line 14 demonstrated that new construction was possible and could achieve standards superior to the historical network. It has since been extended multiple times and serves as the template for future expansion.

The Grand Paris Express, currently under construction, represents the most ambitious expansion since the network's founding. This project will add four new automated lines (15, 16, 17, and 18) encircling the existing network and connecting suburban centers that currently require traveling through central Paris to reach one another. When complete, probably sometime in the 2030s, the Grand Paris Express will nearly double the length of the automated network and fundamentally reshape mobility patterns in the Île-de-France region.

### What makes the Métro work

Several design principles distinguish the Paris Métro from less successful transit systems. First, the high density of stations means that walking to the Métro is almost always faster than driving to a parking lot. This convenience generates ridership that justifies the investment. Second, the integration with other modes is seamless. The same ticket works on Métro, RER, buses, and trams within Paris. Transfer stations connect lines at useful angles rather than requiring passengers to exit one system and enter another. Third, the frequency of service makes timetables irrelevant. During peak hours, trains arrive every two minutes on busy lines. Even late at night, waits rarely exceed ten minutes. Passengers simply show up and go.

The signage and wayfinding deserve particular praise. Station names appear in a consistent typeface (Parisine, designed specifically for the Métro in 1996) on tiled walls visible from passing trains. Corridor signs point toward exits, transfers, and surface landmarks with clarity that serves tourists and commuters alike. The colored line numbers and terminus names provide all the information needed to navigate without consulting maps. Many transit systems aspire to this legibility but few achieve it so thoroughly.

```{r}
metro |>
  dplyr::filter(!is.na(passengers)) |>
  dplyr::group_by(lines) |>
  dplyr::summarize(
    stations = n(),
    total_passengers = sum(passengers, na.rm = TRUE),
    avg_passengers = mean(passengers, na.rm = TRUE),
    .groups = "drop"
  ) |>
  dplyr::slice_max(total_passengers, n = 10) |>
  gt() |>
  fmt_number(columns = c(total_passengers, avg_passengers), suffixing = TRUE) |>
  cols_label(
    lines = "Line(s)",
    stations = "Station Count",
    total_passengers = "Total Ridership",
    avg_passengers = "Avg per Station"
  ) |>
  tab_header(
    title = "Ridership by Line Assignment",
    subtitle = "Stations grouped by their line connections"
  )
```

The Métro also benefits from Paris's urban form. The city is dense and compact, with most destinations within walking distance of a station. Zoning never separated residential from commercial uses as strictly as in American cities, so people live near where they work and shop. The Métro did not create this urban form (it predates the Métro by centuries), but the two reinforce one another. Dense cities need mass transit, and mass transit makes density livable.

For the `metro` dataset, this context matters. The station names are not arbitrary labels but markers of neighborhoods with distinct characters. The ridership figures reflect how Parisians actually move through their city. The line assignments show which routes carry the heaviest loads and which serve more specialized purposes. Understanding the Métro as a living system, constantly adapting over 125 years of operation, makes the dataset more meaningful than raw numbers alone could convey.

### The future of Paris transit

The Grand Paris Express will transform the region, but it is only part of a broader vision. Line 14 continues to extend northward and southward. Line 11 is being extended to connect new suburbs. Tram lines are expanding along the outer boulevards, and bus networks are being reorganized to feed into the rail system more efficiently. The goal is a regional transit network that allows travel between any two points without necessarily passing through central Paris.

The dataset updates periodically to reflect these changes. New stations appear as they open. Ridership figures are updated with each annual release. The `metro` data is not a static snapshot but an evolving portrait of a transit system that continues to grow and adapt. Future versions will include the Grand Paris Express stations, extending coverage far beyond the historical city limits.

For **gt** demonstrations, the dataset provides geographic data with French language station names, offering opportunities to demonstrate locale handling and the presentation of transit network information. The ridership figures support ranking tables. The line assignments (stored as comma-separated values) demonstrate handling of multi-valued fields. The opening dates span over a century, creating interesting timelines. But beyond these technical uses, the dataset offers a window into one of humanity's great collective achievements: a transit system that moves millions of people daily, efficiently and reliably, through one of the world's most beautiful cities.

## `gibraltar`

The `gibraltar` dataset contains hourly weather observations from Gibraltar during May 2023: temperature, humidity, wind speed, cloud cover, and other meteorological variables. It provides 744 rows representing each hour of a single month in this small but fascinating territory.

```{r}
gibraltar |>
  dplyr::slice_sample(n = 8) |>
  dplyr::select(date, time, temp, humidity, wind_speed, wind_dir) |>
  gt() |>
  fmt_number(columns = temp, decimals = 1) |>
  fmt_integer(columns = c(humidity, wind_speed)) |>
  cols_label(
    date = "Date",
    time = "Time", 
    temp = "Temp (°C)",
    humidity = "Humidity %",
    wind_speed = "Wind (km/h)",
    wind_dir = "Direction"
  )
```

Gibraltar sits at the southern tip of the Iberian Peninsula, a British Overseas Territory of barely seven square kilometers guarding the entrance to the Mediterranean Sea. It is the kind of place that captures the imagination precisely because it seems improbable: a limestone promontory with its own airport runway crossing the main road, Barbary macaques roaming the upper rock, and a rather complex history.

### Understanding the Rock

The Rock of Gibraltar rises 426 meters above sea level, a dramatic limestone formation that has served as a strategic landmark for millennia. The ancient Greeks called it one of the Pillars of Hercules, marking the edge of the known world. Every Mediterranean power has recognized its importance: control Gibraltar and you control access between the Atlantic and the Mediterranean. The British acquired it in 1704 during the War of Spanish Succession and have held it ever since, despite periodic Spanish objections and one famous siege that lasted nearly four years.

```{r}
#| echo: false
gibraltar |>
  dplyr::mutate(
    hour = as.integer(sub(":.*", "", time)),
    period = case_when(
      hour >= 6 & hour < 12 ~ "Morning",
      hour >= 12 & hour < 18 ~ "Afternoon",
      hour >= 18 & hour < 22 ~ "Evening",
      TRUE ~ "Night"
    )
  ) |>
  dplyr::group_by(period) |>
  dplyr::summarize(
    avg_temp = mean(temp, na.rm = TRUE),
    max_temp = max(temp, na.rm = TRUE),
    min_temp = min(temp, na.rm = TRUE),
    avg_humidity = mean(humidity, na.rm = TRUE),
    avg_wind = mean(wind_speed, na.rm = TRUE),
    .groups = "drop"
  ) |>
  dplyr::mutate(period = factor(period, levels = c("Morning", "Afternoon", "Evening", "Night"))) |>
  dplyr::arrange(period) |>
  gt(rowname_col = "period") |>
  fmt_number(columns = c(avg_temp, max_temp, min_temp, avg_wind), decimals = 1) |>
  fmt_number(columns = avg_humidity, decimals = 0, pattern = "{x}%") |>
  cols_label(
    avg_temp = "Average",
    max_temp = "Maximum",
    min_temp = "Minimum",
    avg_humidity = "Humidity",
    avg_wind = "Wind"
  ) |>
  tab_header(
    title = "Gibraltar Weather by Time of Day",
    subtitle = "May 2023"
  ) |>
  tab_spanner(
    label = "Temperature (°C)",
    columns = c(avg_temp, max_temp, min_temp)
  )
```

The May weather data captures Gibraltar in spring, before the intense heat of Mediterranean summer arrives. Temperatures climb through the afternoon hours and descend through the evening, following the familiar diurnal pattern. Humidity inversely tracks temperature, rising as the air cools. Wind direction matters at Gibraltar: the Levante wind blows from the east through the strait, often bringing fog as Mediterranean moisture condenses against the Rock. The Poniente arrives from the west, drier and clearer. These wind patterns shaped navigation through the strait for centuries of sailing ships.

```{r}
#| echo: false
gibraltar |>
  dplyr::count(wind_dir) |>
  dplyr::arrange(desc(n)) |>
  gt(rowname_col = "wind_dir") |>
  cols_label(n = "Hours") |>
  tab_header(
    title = "Wind Direction Frequency",
    subtitle = "May 2023"
  )
```

The predominance of certain wind directions reflects the geography of the strait. Air flows through the narrow gap between Europe and Africa, channeled by the mountains on either side. Local topography further complicates matters: the Rock itself creates wind shadows and acceleration zones. Pilots landing at Gibraltar Airport must contend with these effects, making it one of the more challenging airports in Europe. The runway crosses Winston Churchill Avenue, requiring traffic to stop when aircraft land or take off.

May was chosen simply to provide pre-summer weather data. Gibraltar's Mediterranean climate means mild, pleasant conditions that month, with temperatures climbing toward but not yet reaching peak summer heat. The specific year (2023) holds no particular significance beyond being recent enough for the data to feel current. The data comes from weather APIs providing historical observations, typical of the sources that make meteorological data increasingly accessible for analysis and visualization.

For **gt**, the dataset demonstrates time series formatting, weather data presentation, and the handling of multiple related numeric columns. Temperature formatting, wind direction encoding, and the diurnal patterns visible in hourly data all provide teaching opportunities.

## `films`

The `films` dataset is a labor of love: a comprehensive record of every film that has competed for the Palme d'Or at the Cannes Film Festival. It contains 1,607 entries spanning the festival's history, with each row recording a film's title (in both English and original language), director, year, country of origin, spoken languages, and IMDB link.

```{r}
films |>
  dplyr::filter(year >= 2020) |>
  dplyr::select(title, director, countries_of_origin, year) |>
  dplyr::slice_head(n = 10) |>
  gt(groupname_col = "year") |>
  cols_label(
    title = "Film",
    director = "Director",
    countries_of_origin = "Country"
  )
```

The dataset exists because I am a devoted cinephile. My letterboxd account (letterboxd.com/rich_i/) tracks viewing habits and provides an ongoing record of films watched and opinions formed. Film festivals provide endless opportunities for discovery, surfacing works that might never reach mainstream distribution. The Cannes Film Festival, as the most prestigious venue for international cinema, seemed like essential data that should exist in an accessible format. But no such dataset was publicly available. The only solution was to create one.

### Building the Cannes dataset

Construction required extensive research spanning months of work. The festival's official website provided the foundation, listing competition entries by year. But the website alone was insufficient. Many older entries appeared only with French titles, requiring investigation to find corresponding English names (or vice versa for English-language films shown under French titles). Some films had been released under multiple names in different markets, demanding careful verification of which title was authoritative.

```{r}
#| echo: false
films |>
  dplyr::count(year, name = "films") |>
  dplyr::filter(year >= 1970) |>
  dplyr::slice_head(n = 20) |>
  gt(rowname_col = "year") |>
  cols_label(films = "In-Competition Films") |>
  tab_header(
    title = "Cannes Competition Entries by Year",
    subtitle = "Sample of years from 1970 onward"
  )
```

IMDB links were tracked down for each entry, providing viewers easy access to cast lists, synopses, and user ratings. This was straightforward for recent films but required detective work for older or more obscure entries. Some films from the 1950s and 1960s had minimal online presence, with IMDB pages containing little additional information. But the links exist for completeness, allowing interested viewers to explore further.

Spoken languages and countries of origin required the most careful coding. International co-productions muddy the concept of a film's "country". Is a film shot in France, funded by German and Italian producers, directed by a Polish filmmaker, and starring British actors a French film? The dataset records all countries involved in production, accepting that many films defy simple national categorization. Languages posed similar challenges: a film might be primarily in French with scenes in Arabic and English, and all three languages deserve acknowledgment. Where multiple languages appear, I tried to arrange them roughly by the quantity of words spoken in each, so the first language listed is generally the one that dominates the dialogue.

### The festival and its significance

The Cannes Film Festival has operated since 1946 (with a brief predecessor event in 1939 interrupted by war). It functions simultaneously as a trade show for film distribution, a competition for artistic achievement, and a showcase for celebrity culture. The Palme d'Or, awarded to the best film in competition, carries considerable prestige. Winners enter the canon of international cinema, their directors' careers transformed by the recognition.

```{r}
#| echo: false
films |>
  dplyr::count(countries_of_origin, sort = TRUE) |>
  dplyr::slice_head(n = 12) |>
  gt(rowname_col = "countries_of_origin") |>
  cols_label(n = "Films in Competition") |>
  tab_header(
    title = "Countries by Cannes Competition Entries",
    subtitle = "All-time totals across festival history"
  )
```

The table above reveals which national cinemas have received the most recognition at Cannes. France dominates, unsurprisingly given that Cannes is a French festival on the French Riviera. The United States and Italy follow, both countries with robust film industries and strong traditions of auteur filmmaking. Japan's presence reflects the festival's long appreciation for directors like Kurosawa, Ozu, and more recently Kore-eda and Hamaguchi. The geographic diversity of competition entries has increased over time, with films from Korea, Iran, Thailand, and other countries appearing regularly in recent decades.

The festival also reflects changing tastes and priorities in world cinema. In its early decades, Cannes emphasized European art cinema and established masters. The 1970s brought more adventurous programming, with controversial entries and recognition for directors working outside commercial constraints. Recent years have seen increased attention to women directors (historically underrepresented) and to cinemas from regions previously marginalized in international distribution.

```{r}
#| echo: false
films |>
  dplyr::group_by(director) |>
  dplyr::summarize(
    entries = n(),
    years = paste(range(year), collapse = "-"),
    .groups = "drop"
  ) |>
  dplyr::filter(entries >= 5) |>
  dplyr::arrange(desc(entries)) |>
  dplyr::slice_head(n = 10) |>
  gt(rowname_col = "director") |>
  cols_label(
    entries = "Competition Entries",
    years = "Years Active"
  ) |>
  tab_header(
    title = "Most Frequent Cannes Competitors",
    subtitle = "Directors with 5+ competition entries"
  )
```

The directors who return repeatedly to Cannes competition form a roster of international cinema's most celebrated figures. Their repeated presence reflects both the festival's loyalty to directors it has championed and these filmmakers' continued production of work deemed worthy of the world's most competitive showcase. For many, a Cannes premiere represents the peak of artistic recognition, the moment when a new work enters the conversation of global cinema.

### Film as data

The `films` dataset demonstrates that even cultural artifacts can be structured for analysis. Each film becomes a row with attributes: title, year, director, country, language. These attributes support queries that would be tedious to answer through casual browsing. Which directors have competed most often? How has the linguistic diversity of competition entries changed over time? What proportion of recent competitors are first-time Cannes directors versus returning favorites?

For **gt** specifically, `films` demonstrates `fmt_flag()` and `fmt_country()` in realistic contexts. The country codes translate directly to flag icons, creating visual tables that communicate nationality at a glance. The categorical structure (years, directors, countries) provides natural grouping opportunities. The IMDB URLs demonstrate link formatting for external references. It is a dataset that makes beautiful tables almost by accident, because film data is inherently interesting to display.

The dataset updates annually as each new festival adds to the historical record. Every May, the Cannes competition announces its official selection, and those entries will appear in future versions of `films`. The ongoing maintenance reflects both practical utility (keeping examples current) and personal interest (following each year's festival with the attention of a devoted fan).

### My Letterboxd

The `films` dataset exists because I really enjoy movies, and that love extends well beyond festival competition entries. Below is a searchable, sortable table of every film I've watched (and mostly rated) on [Letterboxd](https://letterboxd.com/rich_i/). The data was assembled using scripts in this book's repository (`scripts/scrape-letterboxd.R`), which merge the Letterboxd data export files and fetch director information from individual film pages.

```{r}
letterboxd_films <- readr::read_csv("data/letterboxd_films.csv", show_col_types = FALSE)

films_tbl <- 
  letterboxd_films |>
  dplyr::mutate(
    director = if ("director" %in% names(letterboxd_films)) director else NA_character_
  ) |>
  dplyr::arrange(dplyr::desc(year), film_title) |>
  dplyr::select(film_title, director, year, film_rating)

films_tbl |>
  gt() |>
  cols_label(
    film_title = "Film",
    director = "Director",
    year = "Year",
    film_rating = "Rating"
  ) |>
  fmt(
    columns = film_rating,
    fns = function(x) {
      dplyr::case_when(
        is.na(x) ~ "",
        x %% 1 == 0 ~ strrep("\u2605", x),
        x %% 1 != 0 ~ paste0(strrep("\u2605", floor(x)), "\u00BD")
      )
    }
  ) |>
  sub_missing(missing_text = "") |>
  cols_align(align = "center", columns = c(year, film_rating)) |>
  cols_align(align = "left", columns = film_title) |>
  tab_header(
    title = md("**Rich's Letterboxd**"),
    subtitle = md(paste0(
      "All **", nrow(films_tbl), "** watched films."
    ))
  ) |>
  tab_source_note(
    source_note = md(
      "Source: [letterboxd.com/rich_i](https://letterboxd.com/rich_i/)"
    )
  ) |>
  opt_interactive(
    use_search = TRUE,
    use_sorting = TRUE,
    use_pagination = TRUE,
    page_size_default = 25
  )
```

One detail worth noting is that the star ratings used `fmt()` rather than a pre-formatted text column. This matters for interactive tables because `fmt()` changes only the *display* while preserving the underlying numeric values. When a user clicks the `Rating` column header to sort, the table sorts on the original numbers (`5`, `4.5`, `4`, ...) rather than on rendered text like "★★★★½", which would sort alphabetically and produce nonsensical results. It is a small trick but an important one whenever you need sortable columns with custom formatting in `opt_interactive()` tables!

## `illness`

The `illness` dataset takes a different approach than the others. Rather than modeling behavior or compiling reference data, it reproduces a single table from a published scientific article. The source is "A fatal yellow fever virus infection in China: description and lessons" from *Emerging Microbes & Infections* (July 2016), which documented laboratory test results for a patient who contracted yellow fever during travel to Angola.

```{r}
illness |>
  dplyr::select(test, units, day_3, day_7, day_9) |>
  dplyr::slice_head(n = 10) |>
  gt(rowname_col = "test") |>
  fmt_units(columns = units) |>
  cols_label(
    units = "Units",
    day_3 = "Day 3",
    day_7 = "Day 7",
    day_9 = "Day 9"
  )
```

The article is freely available under a Creative Commons license, making reproduction appropriate. The dataset was created specifically to test **gt**'s `fmt_units()` function and its ability to render scientific unit notation correctly. Medical laboratory results frequently include units like mL, μL, g/dL, U/L, and ×10³/μL that require careful formatting (the last of those being particularly tedious to typeset correctly without dedicated tooling). The question was whether **gt** could faithfully reproduce the original Table 1 from the article.

### Reading laboratory values

Medical laboratory tests generate data that require specialized interpretation. Each test has reference ranges defining normal values, and deviations above or below those ranges signal pathology. A white blood cell count of 3.0 × 10⁹/L might indicate leukopenia (low white cells), potentially signifying infection, bone marrow problems, or medication side effects. Liver enzymes elevated beyond normal ranges suggest hepatic damage. Reading the `illness` dataset means tracking multiple indicators as they evolve day by day through a fatal disease progression.

```{r}
#| echo: false
illness |>
  dplyr::filter(!is.na(norm_u) & !is.na(norm_l)) |>
  dplyr::select(test, units, norm_l, norm_u) |>
  dplyr::slice_head(n = 12) |>
  gt(rowname_col = "test") |>
  cols_label(
    units = "Units",
    norm_l = "Low",
    norm_u = "High"
  ) |>
  tab_spanner(
    label = "Normal Range",
    columns = c(norm_l, norm_u)
  ) |>
  tab_header(title = "Laboratory Test Reference Ranges")
```

The normal ranges provide context for interpreting measurements. When day 9 values fall far outside these ranges, the severity becomes apparent. Bilirubin rising dramatically indicates liver failure. Creatinine elevation signals kidney involvement. The cascade of organ dysfunction visible in sequential laboratory values explains why this case study merited publication and why it serves as a teaching resource.

The dataset thus serves as a benchmark: if you can recreate a published scientific table using **gt**, the package's formatting capabilities are proven sufficient for real-world use. The `illness` data provides that proof of concept while also documenting a tragic case that contributed to medical understanding of yellow fever progression.

## `rx_adsl` and `rx_addv`

These two datasets represent **gt**'s connection to the pharmaceutical industry, where clinical trial tables must meet rigorous standards for regulatory submission. The datasets follow CDISC (Clinical Data Interchange Standards Consortium) conventions, specifically the ADaM (Analysis Data Model) structure used throughout the pharmaceutical industry.

`rx_adsl` contains subject-level data (ADSL format) for 182 participants in a fictional clinical trial. `rx_addv` provides protocol deviation records (ADDV format) with 910 entries documenting when and how trial participants deviated from study protocols. Both datasets use standard variable names and coding conventions that pharmaceutical statisticians will immediately recognize.

```{r}
rx_adsl |>
  dplyr::select(USUBJID, AGE, SEX, ETHNIC, TRTA) |>
  dplyr::slice_head(n = 8) |>
  gt() |>
  cols_label(
    USUBJID = "Subject ID",
    AGE = "Age",
    SEX = "Sex",
    ETHNIC = "Ethnicity",
    TRTA = "Treatment"
  )
```

### The language of clinical trials

Pharmaceutical data follows conventions that seem arcane to outsiders but enable precise communication among specialists. USUBJID uniquely identifies a subject across all studies from a sponsor. TRTA indicates the actual treatment received (as opposed to the treatment assigned). SAFFL flags subjects in the safety population. This vocabulary, defined by CDISC standards, appears in regulatory submissions worldwide. A statistician in Switzerland reviewing a submission from Japan knows exactly what TRTA means because the standards are universal.

```{r}
#| echo: false
rx_adsl |>
  dplyr::group_by(TRTA) |>
  dplyr::summarize(
    subjects = n(),
    avg_age = mean(AGE, na.rm = TRUE),
    pct_female = mean(SEX == "F", na.rm = TRUE) * 100,
    ethnicities = n_distinct(ETHNIC),
    .groups = "drop"
  ) |>
  gt(rowname_col = "TRTA") |>
  fmt_number(columns = avg_age, decimals = 1) |>
  fmt_number(columns = pct_female, decimals = 0, pattern = "{x}%") |>
  cols_label(
    subjects = "Subjects",
    avg_age = "Mean Age",
    pct_female = "Female",
    ethnicities = "Ethnicities"
  ) |>
  tab_header(title = "Treatment Arm Demographics")
```

The treatment arms in clinical trials typically include the experimental treatment at one or more doses, a placebo or active comparator, and sometimes multiple dosing regimens. Demographic balance across arms helps ensure that observed differences reflect treatment effects rather than baseline differences. Age, sex, ethnicity, disease severity at baseline, and prior treatments all require documentation and comparison.

```{r}
#| echo: false
rx_addv |>
  dplyr::count(DVCAT) |>
  dplyr::arrange(desc(n)) |>
  head(8) |>
  gt(rowname_col = "DVCAT") |>
  cols_label(n = "Deviations") |>
  tab_header(title = "Protocol Deviation Categories")
```

Protocol deviations document when trial participants did not follow the study plan. Some deviations are minor (a visit occurring outside the allowed window). Others are major (taking prohibited medications, missing doses). The `rx_addv` dataset catalogs these deviations, enabling sensitivity analyses that exclude subjects with major violations. Regulators scrutinize deviation patterns for evidence that the trial was conducted properly and that deviations do not undermine the conclusions.

These datasets were contributed by Alexandra Lauer as part of ongoing collaboration between **gt** developers and pharmaceutical industry users. The package website includes a dedicated case study article demonstrating how to create clinical tables that meet industry standards. For pharmaceutical statisticians evaluating **gt** for regulatory work, these datasets provide immediately relevant examples.

The inclusion of pharmaceutical data reflects **gt**'s ambition to serve professional communities with specialized requirements. Clinical trials generate enormous quantities of tabular output, much of it following strict formatting conventions. Having sample datasets in the standard format lowers the barrier for pharmaceutical users to adopt **gt** and verify that it meets their needs.

## The value of curated datasets

Looking across all eighteen datasets, certain patterns emerge. Many fill gaps where public data existed but not in convenient form. The scientific datasets consolidate information scattered across journals and government pages. The `films` dataset creates a resource that simply did not exist before. Even the simulated `pizzaplace` data serves a purpose: realistic transactional data is rarely available publicly due to business confidentiality.

Other datasets reflect personal curiosity. Ontario towns. The Paris Métro. Gibraltar's weather. Cannes films. These choices say something about my interests and the particular corners of the world that captured my attention. The datasets are better for this personal investment. Someone who cares about the Paris Métro will notice details that a disinterested compiler would miss.

For users of **gt**, the datasets provide reliable materials for learning and experimentation. The variety ensures that nearly any table type (financial, scientific, demographic, geographic, categorical) has relevant sample data available. The careful construction means edge cases are present: missing values, unusual formatting requirements, multi-valued fields. The documentation grounds each dataset in context that makes the data more meaningful to work with.

Datasets are infrastructure. Good ones get used for years, appearing in examples, tutorials, homework assignments, and documentation. The eighteen datasets in **gt** aspire to that longevity. They are not throwaway data generated to fill a requirement but carefully assembled resources intended to remain useful across many versions and use cases. The stories behind them, now recorded here, add another layer of value: not just what the data contains but why it exists and where it came from.

